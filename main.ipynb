{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeRF: Collision handling in Instant Neural Graphics Primitives\n",
    "#### Federico Montagna (fedemonti00@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda avilable: True\n",
      "Available device 0: NVIDIA GeForce RTX 2070\n",
      "Available device 1: NVIDIA GeForce RTX 2070\n",
      "Current device 0: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from math import atan2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MultipleLocator, AutoMinorLocator, FixedLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections, functools, operator\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import io, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "import traceback\n",
    "from pprint import pprint\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pdb\n",
    "\n",
    "try:\n",
    "    from zoneinfo import ZoneInfo\n",
    "except ImportError:\n",
    "    from backports.zoneinfo import ZoneInfo\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# from params import *\n",
    "\n",
    "print(\"Cuda avilable:\", torch.cuda.is_available())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Available device {i}:\", torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device {torch.cuda.current_device()}:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# torch.set_default_device(device)\n",
    "\n",
    "# random_seed = None\n",
    "# if should_random_seed:\n",
    "#     random_seed = 2**16 - 1\n",
    "#     random.seed(random_seed)\n",
    "#     np.random.seed(random_seed)\n",
    "#     torch.manual_seed(random_seed)\n",
    "#     torch.random.manual_seed(random_seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed_all(random_seed)\n",
    "#     print(\"Random seed:\", random_seed)\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"main.ipynb\"\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "def print2(texts, log: bool = False, color: bcolors = bcolors.OKCYAN) -> None:\n",
    "    if log:\n",
    "        stack = traceback.extract_stack()\n",
    "        calling_frame = stack[-2]\n",
    "        calling_line = calling_frame.line\n",
    "        print(color, \"Line: \", calling_line, bcolors.ENDC)\n",
    "        for text in texts:\n",
    "            print(text)\n",
    "        print(color, \"-\"*20, bcolors.ENDC)\n",
    "\n",
    "\n",
    "def print_allocated_memory(log: bool = True):\n",
    "    if log:\n",
    "        stack = traceback.extract_stack()\n",
    "        calling_frame = stack[-2]\n",
    "        calling_line = calling_frame.line\n",
    "        print(bcolors.HEADER, \"Line: \", calling_line, bcolors.ENDC)\n",
    "\n",
    "        allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to gigabytes\n",
    "        print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert to gigabytes\n",
    "        print(f\"Peak Allocated Memory: {peak_memory:.2f} GB\")\n",
    "\n",
    "        print(bcolors.OKCYAN, \"-\"*20, bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load wandb apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apikey_path = \".wandb_apikey.txt\"\n",
    "# if os.path.exists(apikey_path):\n",
    "#     with open(apikey_path, \"r\") as f:\n",
    "#         apikey = f.read()\n",
    "#         !wandb login {apikey} # --relogin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        dir_name: str,\n",
    "        images_names: List[str],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        root : str\n",
    "            Path to root directory.\n",
    "        dir_name : str\n",
    "            Name of directory.\n",
    "        images_names : List[str]\n",
    "            List of images names.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        self._root: str = root\n",
    "        self._dir_name: str = dir_name\n",
    "        self._images_names: List[str] = images_names\n",
    "\n",
    "        self._images_paths: List[str] = [\n",
    "            os.path.join(self._root, self._dir_name, image_name)\n",
    "            for image_name in self._images_names\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self, idx: torch.Tensor or int) -> Tuple[torch.Tensor, torch.Tensor, int, int]:\n",
    "        \"\"\"\n",
    "        Get data by indices.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : torch.Tensor or int\n",
    "            Indices of data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor, int, int]\n",
    "            Tuple of images.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If images have different sizes.\n",
    "        \"\"\"\n",
    "\n",
    "        if idx == -1:\n",
    "            idx = torch.arange(len(self._images_paths))\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        else:\n",
    "            idx = [idx]\n",
    "\n",
    "        images: List = [\n",
    "            rearrange(io.read_image(self._images_paths[_id]), \"rgb h w -> h w rgb\")\n",
    "            for _id in idx\n",
    "        ]\n",
    "\n",
    "        heights, widths = list(set([image.shape[0] for image in images])), list(set([image.shape[1] for image in images]))\n",
    "\n",
    "        if len(heights) > 1 or len(widths) > 1:\n",
    "            raise ValueError(\"Images have different sizes.\")\n",
    "        else:\n",
    "            h = heights[0]\n",
    "            w = widths[0]\n",
    "\n",
    "        X: torch.Tensor = (\n",
    "            torch.stack([\n",
    "                torch.tensor(\n",
    "                    np.stack(np.meshgrid(range(h), range(w), indexing=\"ij\"), axis=-1).reshape(-1, 2)\n",
    "                )\n",
    "                for _ in images\n",
    "            ]).float() / (max(w, h)) # No (max(w, h) - 1)\n",
    "        ).unsqueeze(-1).unsqueeze(-1) #.requires_grad_()\n",
    "        \n",
    "        Y: torch.Tensor = torch.stack(\n",
    "            images\n",
    "        ).float() / 255\n",
    "\n",
    "        return X, Y, h, w, self._images_names\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self._images_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Convolution Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianConvolution(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor, hash_table_size: int):\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        ctx.save_for_backward(x)\n",
    "        ctx.hash_table_size = hash_table_size\n",
    "\n",
    "        indices = torch.round(x * hash_table_size)\n",
    "\n",
    "        # ctx.indices = indices\n",
    "        # ctx.save_for_backward(indices)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        # indices = ctx.saved_tensors\n",
    "        x, = ctx.saved_tensors\n",
    "        hash_table_size = ctx.hash_table_size\n",
    "\n",
    "        indices = x * hash_table_size\n",
    "        # indices = ctx.indices\n",
    "\n",
    "        grad_x = grad_output * norm.pdf(np.arange(0, hash_table_size, 1), loc=indices, scale=1)\n",
    "\n",
    "        return grad_x, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashFunction(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layers_widths: List[int],\n",
    "        input_dim: int = 2,\n",
    "        output_dim: int = 1,\n",
    "        hash_table_size: int = 2**14,\n",
    "        should_log: int = 0, \n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Hash function module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_layers_widths : List[int]\n",
    "            List of hidden layers widths.\n",
    "        input_dim : int, optional (default is 2)\n",
    "            Input dimension\n",
    "        output_dim : int, optional (default is 1)\n",
    "            Output dimension\n",
    "        hash_table_size : int, optional (default is 2**14)\n",
    "            Hash table size.\n",
    "        should_log : int, optional (default is 0)\n",
    "            - 0: No logging.\n",
    "            - > 0: Log forward pass.\n",
    "            - > 1: Log layers grads and outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        super(HashFunction, self).__init__()\n",
    "\n",
    "        self._hash_table_size: int = hash_table_size\n",
    "        self._should_log: int = should_log\n",
    "\n",
    "        layers_widths: List[int] = [input_dim, *hidden_layers_widths, output_dim]\n",
    "\n",
    "        self.module_list: nn.ModuleList = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=layers_widths[i], out_features=layers_widths[i + 1]),\n",
    "                nn.ReLU() if i < len(layers_widths) - 2 else nn.Sigmoid()\n",
    "            ) for i in range(len(layers_widths) - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        print2((\"Input:\", x, x.shape), (self._should_log > 0))\n",
    "        print2((\"Input grad:\", x.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        for i, layer in enumerate(self.module_list):\n",
    "            x = layer(x)\n",
    "\n",
    "        print2((f\"After layers:\", f\"Output: {x}, shape: {x.shape}\"), (self._should_log > 0))\n",
    "        print2((f\"After layers:\", f\"Grad info: {x.requires_grad}, {x.grad_fn}\"), (self._should_log > 1))\n",
    "        \n",
    "        # x = torch.nan_to_num(x) # Sanitize nan to 0.0\n",
    "\n",
    "        indices = GaussianConvolution.apply(x, self._hash_table_size)\n",
    "        print2((\"GaussianConvolution:\", x, x.shape), (self._should_log > 0))\n",
    "        print2((\"GaussianConvolution grad:\", indices.requires_grad, ), (self._should_log > 1))\n",
    "\n",
    "        return x, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiresolution(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_min: int,\n",
    "        n_max: int,\n",
    "        num_levels: int,\n",
    "        HashFunction: HashFunction,\n",
    "        hash_table_size: int = 2**14,\n",
    "        input_dim: int = 2,\n",
    "        should_log: int = 0\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Multiresolution module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_min : int\n",
    "            Minimum scaling factor.\n",
    "        n_max : int\n",
    "            Maximum scaling factor.\n",
    "        num_levels : int\n",
    "            Number of levels.\n",
    "        HashFunction : HashFunction\n",
    "            Hash function module.\n",
    "        hash_table_size : int, optional (default is 2**14)\n",
    "            Hash table size.\n",
    "        input_dim : int, optional (default is 2)\n",
    "            Input dimension.\n",
    "        should_log : int, optional (default is 0)\n",
    "            - 0: No logging.\n",
    "            - > 0: Log forward pass.\n",
    "            - > 1: Log helper functions.\n",
    "            - > 2: Log collisions.\n",
    "            - > 3: Log dummies.\n",
    "            - > 5: Log initialization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        super(Multiresolution, self).__init__()\n",
    "\n",
    "        self.HashFunction: nn.Module = HashFunction\n",
    "\n",
    "        self._hash_table_size: int = hash_table_size\n",
    "        self._num_levels: int = num_levels\n",
    "        self._input_dim: int = input_dim\n",
    "        self._should_log: int = should_log\n",
    "\n",
    "        b: torch.Tensor = torch.tensor(np.exp((np.log(n_max) - np.log(n_min)) / (self._num_levels - 1))).float()\n",
    "        if b > 2 or b <= 1:\n",
    "            print(\n",
    "                f\"The between level scale is recommended to be <= 2 and needs to be > 1 but was {b:.4f}.\"\n",
    "            )\n",
    "        \n",
    "        self._levels: torch.Tensor = torch.stack([\n",
    "            torch.floor(n_min * (b ** l)) for l in range(self._num_levels)\n",
    "        ]).reshape(1, 1, -1, 1)\n",
    "        print2((\"Levels:\", self._levels, self._levels.shape), (self._should_log > 5))\n",
    "        print2((\"Levels grads:\", self._levels.requires_grad, ), (self._should_log > 5))\n",
    "\n",
    "        self._voxels_helper_hypercube: torch.Tensor = rearrange(\n",
    "            torch.tensor(\n",
    "                np.stack(np.meshgrid(range(2), range(2), range(self._input_dim - 1), indexing=\"ij\"), axis=-1)\n",
    "            ),\n",
    "            \"cols rows depths verts -> (depths rows cols) verts\"\n",
    "        ).T[:self._input_dim, :].unsqueeze(0).unsqueeze(2)\n",
    "        print2((\"voxels_helper_hypercube:\", self._voxels_helper_hypercube, self._voxels_helper_hypercube.shape), (self._should_log > 5))\n",
    "        print2((\"voxels_helper_hypercube grads:\", self._voxels_helper_hypercube.requires_grad), (self._should_log > 5))\n",
    "\n",
    "        self.min_possible_collisions: torch.Tensor = ((self._levels[0, 0, :, 0] + 1) ** 2) - self._hash_table_size\n",
    "        self.min_possible_collisions[self.min_possible_collisions < 0] = 0\n",
    "        print2((\"min_possible_collisions:\", self.min_possible_collisions, self.min_possible_collisions.shape), (self._should_log > 5))\n",
    "        print2((\"min_possible_collisions grads:\", self.min_possible_collisions.requires_grad), (self._should_log > 5))\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        should_calc_hists: bool = False\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        # print2((\"x:\", x, x.shape), (self._should_log > 0))\n",
    "        print2((\"x grads:\", x.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        scaled_coords, grid_coords = self._scale_to_grid(x)\n",
    "\n",
    "        probs, hashed = self.HashFunction(grid_coords)\n",
    "        print2((\"hashed:\", hashed, hashed.shape), (self._should_log > 0))\n",
    "        print2((\"hashed grads:\", hashed.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        print2((\"probs:\", probs, probs.shape), (self._should_log > 0))\n",
    "        print2((\"probs grads:\", probs.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        dummy_grids, dummy_hashed, og_indices = self._calc_dummies(grid_coords, hashed)\n",
    "\n",
    "        collisions: torch.Tensor = self.calc_hash_collisions(dummy_grids, dummy_hashed)\n",
    "        del dummy_hashed\n",
    "\n",
    "        unique_probs, unique_hashed = self._calc_uniques(probs, hashed, og_indices)\n",
    "        del og_indices\n",
    "\n",
    "        if should_calc_hists:\n",
    "            hists = self._hist_collisions(dummy_grids, unique_hashed, should_show=False)\n",
    "        else:\n",
    "            hists = None\n",
    "        del dummy_grids, unique_hashed\n",
    "\n",
    "        return hashed, unique_probs, collisions, hists\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _scale_to_grid(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Scale coordinates to grid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Original coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]\n",
    "            Scaled coordinates, grid coordinates and dummy grids.\n",
    "        \"\"\"\n",
    "\n",
    "        scaled_coords: torch.Tensor = x.float() * self._levels.float()\n",
    "        print2((\"scaled_coords:\", scaled_coords, scaled_coords.shape), (self._should_log > 1))\n",
    "        print2((\"scaled_coords grads:\", scaled_coords.requires_grad, ), (self._should_log > 1))\n",
    "\n",
    "        grid_coords: torch.Tensor = rearrange(\n",
    "            torch.add(\n",
    "                torch.floor(scaled_coords),\n",
    "                self._voxels_helper_hypercube\n",
    "            ),\n",
    "            \"batch pixels xyz levels verts -> batch pixels levels verts xyz\"\n",
    "        )\n",
    "        print2((\"grid_coords:\", grid_coords, grid_coords.shape), (self._should_log > 1))\n",
    "        print2((\"grid_coords grads:\", grid_coords.requires_grad, ), (self._should_log > 1))\n",
    "\n",
    "        return scaled_coords, grid_coords\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def calc_hash_collisions(\n",
    "        self, \n",
    "        dummy_grids: List[torch.Tensor], \n",
    "        dummy_hashed: List[torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate hash collisions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dummy_grid_coords : List[torch.Tensor]\n",
    "            Grid coordinates.\n",
    "        dummy_hashed : List[torch.Tensor]\n",
    "            Hashed grid coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Hash collisions per level.\n",
    "        \"\"\"\n",
    "        collisions: torch.Tensor = torch.stack([\n",
    "            # [\n",
    "            torch.tensor(float(dummy_grids[l].shape[0] - torch.unique(dummy_hashed[l], dim=0).shape[0]), requires_grad=True)\n",
    "            for l in range(self._num_levels)\n",
    "            # ] \n",
    "            # for b in range(hashed.shape[0])\n",
    "        ])\n",
    "        print2((\"collisions:\", collisions, collisions.shape, collisions.dtype), (self._should_log > 2))\n",
    "        print2((\"collisions grads:\", collisions.requires_grad, ), (self._should_log > 2))\n",
    "\n",
    "        return collisions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _calc_dummies(\n",
    "        self,\n",
    "        grid_coords: torch.Tensor,\n",
    "        hashed: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate dummies.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid_coords : torch.Tensor\n",
    "            Grid coordinates.\n",
    "        hashed : torch.Tensor\n",
    "            Hashed grid coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor]\n",
    "            Dummy grids and dummy hashed and original unique indices.\n",
    "        \"\"\"\n",
    "\n",
    "        dummy_grids: List[Tuple[torch.Tensor]] = [\n",
    "            # [\n",
    "            torch.unique(rearrange(grid_coords[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\"), dim=0, return_inverse=True) # first dimension is 0 because images should have all same size\n",
    "            for l in range(self._num_levels)\n",
    "            # ] for b in range(grid_coords.shape[0])\n",
    "        ]\n",
    "\n",
    "        dummy_grids, dummy_grids_inverse_indices = zip(*dummy_grids)\n",
    "        print2((\"dummy_grids:\", dummy_grids, [dummy_grids[l].shape for l in range(self._num_levels)]), (self._should_log > 1))\n",
    "        print2((\"dummy_grids grads:\", [(dummy_grids[l].requires_grad, ) for l in range(self._num_levels)]), (self._should_log > 3))\n",
    "\n",
    "        og_indices = [\n",
    "            torch.tensor([np.where(dummy_grids_inverse_indices[l].numpy() == i)[0][0] for i in range(len(dummy_grids[l]))])\n",
    "            for l in range(self._num_levels)\n",
    "        ]\n",
    "        print2((\"og_indices:\", [(og_indices[l], og_indices[l].shape) for l in range(self._num_levels)]), (self._should_log > 1))\n",
    "        print2((\"og_indices grads:\", [(og_indices[l].requires_grad, ) for l in range(self._num_levels)]), (self._should_log > 1))\n",
    "\n",
    "        dummy_hashed: List[torch.Tensor] = [\n",
    "            # [\n",
    "            torch.unique(rearrange(hashed[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\"), dim=0, return_inverse=False) # first dimension is 0 because images should have all same size\n",
    "            for l in range(self._num_levels)\n",
    "            # ] for b in range(hashed.shape[0])\n",
    "        ]\n",
    "        print2((\"dummy_hashed:\", dummy_hashed, [dummy_hashed[l].shape for l in range(self._num_levels)]), (self._should_log > 3))\n",
    "        print2((\"dummy_hashed grads:\", [(dummy_hashed[l].requires_grad, ) for l in range(self._num_levels)]), (self._should_log > 3))\n",
    "\n",
    "        return dummy_grids, dummy_hashed, og_indices\n",
    "    \n",
    "    # @torch.no_grad()\n",
    "    def _calc_uniques(\n",
    "        self,\n",
    "        probs: torch.Tensor,\n",
    "        hashed: torch.Tensor,\n",
    "        og_indices: List[torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate uniques.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probs : torch.Tensor\n",
    "            Probabilities.\n",
    "        hashed : torch.Tensor\n",
    "            Hashed grid coordinates.\n",
    "        og_indices : List[torch.Tensor]\n",
    "            Original unique indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor]\n",
    "            Unique probabilities and unique hashed.\n",
    "        \"\"\"\n",
    "\n",
    "        unique_probs: List[torch.Tensor] = [\n",
    "            # [\n",
    "            F.softmax(rearrange(probs[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\")[og_indices[l]], dim=0)\n",
    "            for l in range(self._num_levels)\n",
    "            # ]\n",
    "            # for b in range(probs.shape[0])\n",
    "        ]\n",
    "        print2((\"unique_probs:\", [(unique_probs[b][l], unique_probs[b][l].shape) for l in range(self._num_levels) for b in range(probs.shape[0])]), (self._should_log > 0))\n",
    "        print2((\"unique_probs:\", [(unique_probs[l], unique_probs[l].shape) for l in range(self._num_levels)]), (self._should_log > 0))\n",
    "\n",
    "        unique_hashed: List[torch.Tensor] = [\n",
    "            # [\n",
    "            rearrange(hashed[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\")[og_indices[l]]\n",
    "            for l in range(self._num_levels)\n",
    "            # ]\n",
    "            # for b in range(probs.shape[0])\n",
    "        ]\n",
    "        print2((\"unique_hashed:\", [(unique_hashed[b][l], unique_hashed[b][l].shape) for l in range(self._num_levels) for b in range(probs.shape[0])]), (self._should_log > 0))\n",
    "        print2((\"unique_hashed:\", [(unique_hashed[l], unique_hashed[l].shape) for l in range(self._num_levels)]), (self._should_log > 0))\n",
    "\n",
    "        return unique_probs, unique_hashed\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _hist_collisions(\n",
    "        self,\n",
    "        dummy_grids: List[torch.Tensor],\n",
    "        dummy_hashed: List[torch.Tensor],\n",
    "        should_show: bool = False\n",
    "    ) -> List[plt.Figure]:\n",
    "        \"\"\"\n",
    "        Show collisions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dummy_grids : List[torch.Tensor]\n",
    "            Grid coordinates.\n",
    "        dummy_hashed : List[torch.Tensor]\n",
    "            Hashed grid coordinates.\n",
    "        should_show : bool, optional (default is False)\n",
    "            Whether to show figure.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[plt.Figure]\n",
    "            Histograms of collisions, one per level.\n",
    "        \"\"\"\n",
    "\n",
    "        figs=[]\n",
    "\n",
    "        for l in range(self._num_levels):\n",
    "\n",
    "            indices = dummy_hashed[l].detach().numpy()\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(15, 5))\n",
    "            ax.hist(\n",
    "                indices,\n",
    "                bins=self._hash_table_size,\n",
    "                range=(0, self._hash_table_size),\n",
    "                edgecolor='grey', \n",
    "                linewidth=0.5\n",
    "            )\n",
    "\n",
    "            ax.set_xlim(-1, self._hash_table_size)\n",
    "            ax.xaxis.set_ticks(np.arange(0, self._hash_table_size, 10))\n",
    "\n",
    "            start, end = ax.get_ylim()\n",
    "            step = int(end * 0.1)\n",
    "            ax.yaxis.set_ticks(np.arange(0, end, step if step > 0 else 1))\n",
    "\n",
    "            plt.title(f\"Level {l} ({int(self._levels[0, 0, l, 0].item())})\")\n",
    "            plt.xlabel(\"Hashed indices\")\n",
    "            plt.ylabel(\"Counts\")\n",
    "\n",
    "            figs.append(fig)\n",
    "\n",
    "            if should_show:\n",
    "                plt.show()\n",
    "            \n",
    "            plt.close()\n",
    "\n",
    "        return figs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics, Loss & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        delta: float = 1,\n",
    "        should_log: int = 0\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Loss module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : float, optional (default is 1)\n",
    "            Delta parameter for collision loss.\n",
    "        should_log : int, optional (default is 0)\n",
    "            - 0: No logging.\n",
    "            - > 0: Log forward pass.\n",
    "            - > 1: Log helper functions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "        self._delta = delta\n",
    "        self._should_log = should_log\n",
    "\n",
    "        self.kl_div_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        collisions: List,\n",
    "        min_possible_collisions: List,\n",
    "        probs: List,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "\n",
    "        # min_possible_collisions[min_possible_collisions <= 0] = self._delta\n",
    "        delta = min_possible_collisions.clone()\n",
    "        delta[delta <= 0] = self._delta\n",
    "    \n",
    "        collisions_losses: torch.Tensor = (collisions - min_possible_collisions) / delta # min_possible_collisions\n",
    "        print2((\"Collisions Losses:\", collisions_losses), self._should_log > 0)\n",
    "        print2((\"Collisions Losses grads:\", collisions_losses.requires_grad), self._should_log > 0)\n",
    "\n",
    "        kl_div_losses = torch.stack([\n",
    "            self._kl_div(prob.shape[0], prob)\n",
    "            for prob in probs\n",
    "        ])\n",
    "\n",
    "        return collisions_losses, kl_div_losses\n",
    "\n",
    "    def _kl_div(\n",
    "        self,\n",
    "        level: int,\n",
    "        p: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate KL divergence loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        level : int\n",
    "            Level.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            KL divergence loss.\n",
    "        \"\"\"\n",
    "\n",
    "        q = torch.ones(level) / level\n",
    "\n",
    "        return self.kl_div_loss(p.log(), q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(\n",
    "    net: torch.nn.Module,\n",
    "    # encoding_lr: float,\n",
    "    hash_lr: float,\n",
    "    # MLP_lr: float,\n",
    "    # encoding_weight_decay: float,\n",
    "    hash_weight_decay: float,\n",
    "    # MLP_weight_decay: float,\n",
    "    betas: tuple = (0.9, 0.99),\n",
    "    eps: float = 1e-15\n",
    "):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            # {\"params\": net.encoding.parameters(), \"lr\": encoding_lr, \"weight_decay\": encoding_weight_decay},\n",
    "            {\"params\": net.HashFunction.parameters(), \"lr\": hash_lr, \"weight_decay\": hash_weight_decay},\n",
    "            # {\"params\": net.mlp.parameters(), \"lr\": MLP_lr, \"weight_decay\": MLP_weight_decay}\n",
    "        ],\n",
    "        betas=betas,\n",
    "        eps=eps,\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, tolerance: int = 5, min_delta: int = 0, should_reset: bool = True):\n",
    "        self.tolerance: int = tolerance\n",
    "        self.min_delta: int = min_delta\n",
    "        self.best_loss: float = np.inf\n",
    "        self.counter: int = 0\n",
    "        self.early_stop: bool = False\n",
    "        self._should_reset: bool = should_reset\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        # print(f\"best_loss: {self.best_loss}, loss: {loss}, counter: {self.counter}\")\n",
    "\n",
    "        if abs(self.best_loss - loss) < self.min_delta and (loss < self.best_loss):\n",
    "            # print(\"Stall\")\n",
    "            self.counter += 1\n",
    "        elif abs(self.best_loss - loss) > self.min_delta and (loss > self.best_loss):\n",
    "            # print(\"Growing\")\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            if not self._should_reset:\n",
    "                if self.counter <= 0:\n",
    "                    self.counter = 0\n",
    "                else:\n",
    "                    self.counter -= 1\n",
    "            else:\n",
    "                self.counter = 0\n",
    "                self.best_loss = loss\n",
    "\n",
    "        if self.counter >= self.tolerance:\n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: 20231102100528\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./images\"\n",
    "test_size = 0.2\n",
    "random_state = 65535\n",
    "\n",
    "train_images, test_images = train_test_split(\n",
    "    [\n",
    "        file for file in os.listdir(root_dir) if (\"silhouette\" in file) and (file.endswith(\".jpg\") or file.endswith(\".png\") or file.endswith(\".jpeg\"))\n",
    "    ], \n",
    "    test_size=test_size, \n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "wandb_entity = \"fedemonti00\"\n",
    "wandb_project = \"project_course\"\n",
    "# wandb_name = \"hash_function_training\"\n",
    "\n",
    "try:\n",
    "    time = wandb_name\n",
    "except NameError:\n",
    "    time = (datetime.now(ZoneInfo(\"Europe/Rome\"))).strftime(\"%Y%m%d%H%M%S\")\n",
    "print(\"RUN:\", time)\n",
    "\n",
    "early_stopper_tolerance = 10\n",
    "early_stopper_min_delta = 1e-6     \n",
    "\n",
    "histogram_rate = 10\n",
    "\n",
    "hyperparameters = {\n",
    "    \"hash_table_size\": 2**8,\n",
    "    \"n_min\": 8,\n",
    "    \"n_max\": 32,\n",
    "    \"num_levels\": 4,\n",
    "    \"output_dim\": 1,\n",
    "    \"hash_function_hidden_layers_widths\": [8, 32, 8],\n",
    "    \"hash_lr\": 1e-3,\n",
    "    \"hash_weight_decay\": 1e-6,\n",
    "    \"l_collisions\": 1,\n",
    "    \"l_kl_loss\": 100,\n",
    "    \"epochs\": 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfedemonti00\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/federicomontagna/collision_handling_in_instantNGP/wandb/run-20231102_100529-0fpna1g5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fedemonti00/project_course/runs/0fpna1g5' target=\"_blank\">20231102100528</a></strong> to <a href='https://wandb.ai/fedemonti00/project_course' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fedemonti00/project_course' target=\"_blank\">https://wandb.ai/fedemonti00/project_course</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fedemonti00/project_course/runs/0fpna1g5' target=\"_blank\">https://wandb.ai/fedemonti00/project_course/runs/0fpna1g5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fedemonti00/project_course/runs/0fpna1g5?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5dfe1c3ac0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------ #\n",
    "#          WANDB INIT            #\n",
    "# ------------------------------ #\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    entity = wandb_entity,\n",
    "    # set the wandb project where this run will be logged\n",
    "    project = wandb_project,\n",
    "\n",
    "    name = time,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config = hyperparameters,\n",
    "    # config = {\n",
    "    #     \"id_grid_search_params\":        id_param,\n",
    "    #     \"grid_search_params\":           params,\n",
    "    #     \"random_seed\":                  random_seed,\n",
    "    #     \"HPD_learning_rate\":            HPD_lr,\n",
    "    #     \"encoding_learning_rate\":       encoding_lr,\n",
    "    #     \"MLP_learning_rate\":            MLP_lr,\n",
    "    #     \"encoding_weight_decay\":        encoding_weight_decay,\n",
    "    #     \"HPD_weight_decay\":             HPD_weight_decay,\n",
    "    #     \"MLP_weight_decay\":             MLP_weight_decay,\n",
    "    #     \"batch_size%\":                  batch_size,\n",
    "    #     \"shuffled_pixels\":              should_shuffle_pixels,\n",
    "    #     \"normalized_data\":              True if not should_batchnorm_data else \"BatchNorm1d\",\n",
    "    #     \"architecture\":                 \"GeneralNeuralGaugeFields\",\n",
    "    #     \"dataset\":                      image_name,\n",
    "    #     \"epochs\":                       epochs,\n",
    "    #     \"color\":                        'RGB' if not should_bw else 'BW',\n",
    "    #     \"hash_table_size\":              hash_table_size,\n",
    "    #     \"num_levels\":                   num_levels,\n",
    "    #     \"n_min\":                        n_min,\n",
    "    #     \"n_max\":                        n_max,\n",
    "    #     \"MLP_hidden_layers_widths\":     str(MLP_hidden_layers_widths),\n",
    "    #     \"HPD_hidden_layers_widths\":     str(HPD_hidden_layers_widths),\n",
    "    #     \"HPD_out_features\":             HPD_out_features,\n",
    "    #     \"feature_dim\":                  feature_dim,\n",
    "    #     \"topk_k\":                       topk_k,\n",
    "    #     \"loss_type\":                    \"JS+KLDiv\" if should_sum_js_kl_div else (\"KLDiv\" if not should_js_div else \"JSDiv\"),\n",
    "    #     \"loss_lambda_MSE\":              l_mse,\n",
    "    #     \"loss_lambda_JS_KL\":            l_js_kl,\n",
    "    #     \"loss_lambda_collisions\":       l_collisions,\n",
    "    #     \"loss_gamma\":                   loss_gamma,\n",
    "    #     \"loss_epsilon\":                 loss_epsilon,\n",
    "    #     \"inplace_scatter\":              should_inplace_scatter,\n",
    "    #     \"MLP_activations\":              \"LeakyReLU\" if should_leaky_relu else \"ReLU\",\n",
    "    #     \"collisions_loss_probs\":        \"topk_only\" if should_keep_topk_only else \"hash_table_size\",\n",
    "    #     \"avg_topk_features\":            \"softmax_avg\" if should_softmax_topk_features else (\"weighted_avg\" if should_softmax_topk_features != None else None),\n",
    "    #     \"hash_type\":                    \"HPD\" if not should_use_hash_function else \"hash_function\"\n",
    "    # }\n",
    "\n",
    "    save_code = True,\n",
    ")\n",
    "\n",
    "# ------------------------------ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImagesDataset(\n",
    "    root=root_dir.split(\"/\")[0],\n",
    "    dir_name=root_dir.split(\"/\")[1],\n",
    "    images_names=train_images\n",
    ")\n",
    "x, y, h, w, names = train_dataset[-1]\n",
    "\n",
    "test_dataset = ImagesDataset(\n",
    "    root=root_dir.split(\"/\")[0],\n",
    "    dir_name=root_dir.split(\"/\")[1],\n",
    "    images_names=test_images\n",
    ")\n",
    "eval_x, eval_y, eval_h, eval_w, eval_names = test_dataset[-1]\n",
    "\n",
    "input_dim = x.shape[-3]\n",
    "should_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiresolution(\n",
      "  (HashFunction): HashFunction(\n",
      "    (module_list): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=8, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=8, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hashFunction = HashFunction(\n",
    "    hidden_layers_widths=hyperparameters[\"hash_function_hidden_layers_widths\"],\n",
    "    input_dim=input_dim,\n",
    "    output_dim=hyperparameters[\"output_dim\"],\n",
    "    hash_table_size=hyperparameters[\"hash_table_size\"],\n",
    "    should_log=0 if should_log else 0\n",
    ")\n",
    "\n",
    "multires = Multiresolution(\n",
    "    n_min=hyperparameters[\"n_min\"],\n",
    "    n_max=hyperparameters[\"n_max\"],\n",
    "    num_levels=hyperparameters[\"num_levels\"],\n",
    "    HashFunction=hashFunction,\n",
    "    hash_table_size=hyperparameters[\"hash_table_size\"],\n",
    "    input_dim=input_dim,\n",
    "    should_log=6 if should_log else 0\n",
    ")\n",
    "\n",
    "loss_fn = Loss(\n",
    "    should_log=0 if should_log else 0\n",
    ")\n",
    "\n",
    "optimizer = get_optimizer(\n",
    "    net=multires,\n",
    "    hash_lr=hyperparameters[\"hash_lr\"],\n",
    "    hash_weight_decay=hyperparameters[\"hash_weight_decay\"],\n",
    ")\n",
    "\n",
    "early_stopper = EarlyStopper(\n",
    "    tolerance=early_stopper_tolerance,\n",
    "    min_delta=early_stopper_min_delta\n",
    ")\n",
    "\n",
    "print(multires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75, Loss: 131.01731872558594, Collisions: tensor([ 38.,  95., 322., 967.]):   8%|▊         | 75/1000 [00:58<11:55,  1.29it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Stopping at epoch: 75 !!!\n"
     ]
    }
   ],
   "source": [
    "plt.ioff()\n",
    "should_calc_hists = False\n",
    "\n",
    "pbar = tqdm(range(0, hyperparameters[\"epochs\"]))\n",
    "\n",
    "multires.train()\n",
    "\n",
    "for e in pbar:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calc histograns at: first epoch, last epoch, every histogram_rate epochs, every time early stopper stops\n",
    "    should_calc_hists = (e == hyperparameters[\"epochs\"] - 1) or (e % histogram_rate == 0) or early_stopper.early_stop\n",
    "    \n",
    "    out, unique_probs, collisions, figs = multires(\n",
    "        x, \n",
    "        should_calc_hists=should_calc_hists \n",
    "    )\n",
    "\n",
    "    collisions_losses, kl_losses = loss_fn(collisions, multires.min_possible_collisions, unique_probs)\n",
    "\n",
    "    collisions_loss = torch.sum(collisions_losses)\n",
    "    kl_loss = kl_losses.sum()\n",
    "    loss = hyperparameters[\"l_collisions\"] * collisions_loss - hyperparameters[\"l_kl_loss\"] * kl_loss\n",
    "    # print2((\"Loss grads:\", loss.requires_grad, ), True, bcolors.HEADER)\n",
    "\n",
    "    log = {\n",
    "        \"train_loss\": loss.item(),\n",
    "    }\n",
    "\n",
    "    for l in range(hyperparameters[\"num_levels\"]):\n",
    "        log[f\"train_collisions_level_{l}\"] = collisions[l].item()\n",
    "        log[f\"train_min_possible_collisions_level_{l}\"] = multires.min_possible_collisions[l].item()\n",
    "\n",
    "        log[f\"train_collisions_loss_level_{l}\"] = collisions_losses[l].item()\n",
    "        log[f\"train_kl_loss_level_{l}\"] = kl_losses[l].item()\n",
    "        \n",
    "        if should_calc_hists:\n",
    "            log[f\"train_hist_counts_level_{l}\"] = wandb.Image(\n",
    "                figs[l],\n",
    "                caption=f\"Hashed indices counts at level {l} at epoch {e}\"\n",
    "            )\n",
    "    del figs\n",
    "\n",
    "    wandb.log(log)\n",
    "\n",
    "    # print2(\n",
    "    #     (f\"\"\"Epoch {e}, \n",
    "    #     Collision_losses: {collisions_losses}, \n",
    "    #     KL Loss: {kl_losses}, \n",
    "    #     Loss: {loss.item()},\n",
    "    #     Collisions: {collisions},\n",
    "    #     Min possible Collisions: {multires.min_possible_collisions}\"\"\", ), \n",
    "    #     True, \n",
    "    #     bcolors.OKGREEN\n",
    "    # )\n",
    "    pbar.set_description(f\"Epoch {e}, Loss: {loss.item()}, Collisions: {collisions}\")\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # for name, param in multires.named_parameters():\n",
    "    #     print(f'Parameter: {name}, Gradient: {param.grad}')\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"!!! Stopping at epoch:\", e, \"!!!\")\n",
    "        break\n",
    "\n",
    "    early_stopper(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_collisions_level_0</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▄▄▄▅▄▃▃▃▃▂▂▂▂▃▃▂▂▂▂▁▁▁▁▂▃▄</td></tr><tr><td>train_collisions_level_1</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▂▂▃</td></tr><tr><td>train_collisions_level_2</td><td>██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train_collisions_level_3</td><td>██▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▃▂▂▂▃▃▂▂▂▃▃▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train_collisions_loss_level_0</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▄▄▄▅▄▃▃▃▃▂▂▂▂▃▃▂▂▂▂▁▁▁▁▂▃▄</td></tr><tr><td>train_collisions_loss_level_1</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▂▂▃</td></tr><tr><td>train_collisions_loss_level_2</td><td>██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train_collisions_loss_level_3</td><td>██▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▃▂▂▂▃▃▂▂▂▃▃▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train_kl_loss_level_0</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_kl_loss_level_1</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇██</td></tr><tr><td>train_kl_loss_level_2</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇█</td></tr><tr><td>train_kl_loss_level_3</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇█</td></tr><tr><td>train_loss</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▃▃</td></tr><tr><td>train_min_possible_collisions_level_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_min_possible_collisions_level_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_min_possible_collisions_level_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_min_possible_collisions_level_3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_collisions_level_0</td><td>38.0</td></tr><tr><td>train_collisions_level_1</td><td>95.0</td></tr><tr><td>train_collisions_level_2</td><td>322.0</td></tr><tr><td>train_collisions_level_3</td><td>967.0</td></tr><tr><td>train_collisions_loss_level_0</td><td>38.0</td></tr><tr><td>train_collisions_loss_level_1</td><td>95.0</td></tr><tr><td>train_collisions_loss_level_2</td><td>0.74054</td></tr><tr><td>train_collisions_loss_level_3</td><td>0.16086</td></tr><tr><td>train_kl_loss_level_0</td><td>0.00601</td></tr><tr><td>train_kl_loss_level_1</td><td>0.00829</td></tr><tr><td>train_kl_loss_level_2</td><td>0.00855</td></tr><tr><td>train_kl_loss_level_3</td><td>0.00599</td></tr><tr><td>train_loss</td><td>131.01732</td></tr><tr><td>train_min_possible_collisions_level_0</td><td>0.0</td></tr><tr><td>train_min_possible_collisions_level_1</td><td>0.0</td></tr><tr><td>train_min_possible_collisions_level_2</td><td>185.0</td></tr><tr><td>train_min_possible_collisions_level_3</td><td>833.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">20231102100528</strong> at: <a href='https://wandb.ai/fedemonti00/project_course/runs/0fpna1g5' target=\"_blank\">https://wandb.ai/fedemonti00/project_course/runs/0fpna1g5</a><br/> View job at <a href='https://wandb.ai/fedemonti00/project_course/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMjA3NjE2MQ==/version_details/v0' target=\"_blank\">https://wandb.ai/fedemonti00/project_course/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMjA3NjE2MQ==/version_details/v0</a><br/>Synced 6 W&B file(s), 36 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231102_100529-0fpna1g5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_course",
   "language": "python",
   "name": "project_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
