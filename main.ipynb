{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeRF: Collision handling in Instant Neural Graphics Primitives\n",
    "#### Federico Montagna (fedemonti00@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda avilable: True\n",
      "Available device 0: NVIDIA GeForce RTX 2070\n",
      "Available device 1: NVIDIA GeForce RTX 2070\n",
      "Current device 0: NVIDIA GeForce RTX 2070\n",
      "Random seed: 65535\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from math import atan2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MultipleLocator, AutoMinorLocator, FixedLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections, functools, operator\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import io, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "import traceback\n",
    "from pprint import pprint\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pdb\n",
    "\n",
    "try:\n",
    "    from zoneinfo import ZoneInfo\n",
    "except ImportError:\n",
    "    from backports.zoneinfo import ZoneInfo\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# from params import *\n",
    "\n",
    "print(\"Cuda avilable:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Available device {i}:\", torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device {torch.cuda.current_device()}:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "torch.set_default_device(device)\n",
    "\n",
    "random_seed = 2**16 - 1\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.random.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "print(\"Random seed:\", random_seed)\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"main.ipynb\"\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "def print2(texts, log: bool = False, color: bcolors = bcolors.OKCYAN) -> None:\n",
    "    if log:\n",
    "        stack = traceback.extract_stack()\n",
    "        calling_frame = stack[-2]\n",
    "        calling_line = calling_frame.line\n",
    "        print(color, \"Line: \", calling_line, bcolors.ENDC)\n",
    "        for text in texts:\n",
    "            print(text)\n",
    "        print(color, \"-\"*20, bcolors.ENDC)\n",
    "\n",
    "\n",
    "def print_allocated_memory(log: bool = True):\n",
    "    if log:\n",
    "        stack = traceback.extract_stack()\n",
    "        calling_frame = stack[-2]\n",
    "        calling_line = calling_frame.line\n",
    "        print(bcolors.HEADER, \"Line: \", calling_line, bcolors.ENDC)\n",
    "\n",
    "        allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to gigabytes\n",
    "        print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert to gigabytes\n",
    "        print(f\"Peak Allocated Memory: {peak_memory:.2f} GB\")\n",
    "\n",
    "        print(bcolors.OKCYAN, \"-\"*20, bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load wandb apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apikey_path = \".wandb_apikey.txt\"\n",
    "# if os.path.exists(apikey_path):\n",
    "#     with open(apikey_path, \"r\") as f:\n",
    "#         apikey = f.read()\n",
    "#         !wandb login {apikey} # --relogin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        dir_name: str,\n",
    "        images_names: List[str],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        root : str\n",
    "            Path to root directory.\n",
    "        dir_name : str\n",
    "            Name of directory.\n",
    "        images_names : List[str]\n",
    "            List of images names.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        self._root: str = root\n",
    "        self._dir_name: str = dir_name\n",
    "        self._images_names: List[str] = images_names\n",
    "\n",
    "        self._images_paths: List[str] = [\n",
    "            os.path.join(self._root, self._dir_name, image_name)\n",
    "            for image_name in self._images_names\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self, idx: torch.Tensor or int) -> Tuple[torch.Tensor, torch.Tensor, int, int]:\n",
    "        \"\"\"\n",
    "        Get data by indices.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : torch.Tensor or int\n",
    "            Indices of data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor, int, int]\n",
    "            Tuple of images.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If images have different sizes.\n",
    "        \"\"\"\n",
    "\n",
    "        if idx == -1:\n",
    "            idx = torch.arange(len(self._images_paths))\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        else:\n",
    "            idx = [idx]\n",
    "\n",
    "        images: List = [\n",
    "            rearrange(io.read_image(self._images_paths[_id]), \"rgb h w -> h w rgb\")\n",
    "            for _id in idx\n",
    "        ]\n",
    "\n",
    "        heights, widths = list(set([image.shape[0] for image in images])), list(set([image.shape[1] for image in images]))\n",
    "\n",
    "        if len(heights) > 1 or len(widths) > 1:\n",
    "            raise ValueError(\"Images have different sizes.\")\n",
    "        else:\n",
    "            h = heights[0]\n",
    "            w = widths[0]\n",
    "\n",
    "        X: torch.Tensor = (\n",
    "            torch.stack([\n",
    "                torch.tensor(\n",
    "                    np.stack(np.meshgrid(range(h), range(w), indexing=\"ij\"), axis=-1).reshape(-1, 2)\n",
    "                )\n",
    "                for _ in images\n",
    "            ]).float() / (max(w, h)) # No (max(w, h) - 1)\n",
    "        ).unsqueeze(-1).unsqueeze(-1) #.requires_grad_()\n",
    "        \n",
    "        Y: torch.Tensor = torch.stack(\n",
    "            images\n",
    "        ).float() / 255\n",
    "\n",
    "        return X, Y, h, w, self._images_names\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self._images_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass Differentiable Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPDA(x, round_function):\n",
    "    forward_value = round_function(x)\n",
    "    out = x.clone()\n",
    "    out.data = forward_value.data\n",
    "    return out\n",
    "\n",
    "def differentiable_floor(x, round_function=torch.floor):\n",
    "    return BPDA(x, round_function)\n",
    "\n",
    "def differentiable_round(x, round_function=torch.round):\n",
    "    return BPDA(x, round_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Convolution Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianConvolution(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor, hash_table_size: int):\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        ctx.save_for_backward(x)\n",
    "        ctx.hash_table_size = hash_table_size\n",
    "\n",
    "        indices = differentiable_round(x * hash_table_size)\n",
    "\n",
    "        # ctx.indices = indices\n",
    "        # ctx.save_for_backward(indices)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        # indices = ctx.saved_tensors\n",
    "        x, = ctx.saved_tensors\n",
    "        hash_table_size = ctx.hash_table_size\n",
    "\n",
    "        indices = x * hash_table_size\n",
    "        # indices = ctx.indices\n",
    "\n",
    "        grad_x = grad_output * norm.pdf(np.arange(0, hash_table_size, 1), loc=indices, scale=1)\n",
    "\n",
    "        return grad_x, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentiableHistogram(nn.Module):\n",
    "    def __init__(self, num_bins):\n",
    "        super(DifferentiableHistogram, self).__init__()\n",
    "        self.num_bins = num_bins\n",
    "        self.register_buffer('counts', torch.zeros(num_bins))\n",
    "\n",
    "        # self.hist_grad = torch.zeros(num_bins)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Compute the histogram\n",
    "        # hist = torch.histc(input, bins=self.num_bins, min=0, max=self.num_bins) # ? maybe max=(self.num_bins - 1)\n",
    "        hist = torch.unique(input).int().bincount(minlength=self.num_bins)\n",
    "        # hist = hist.requires_grad_()\n",
    " \n",
    "        # Update the counts buffer\n",
    "        self.counts += hist\n",
    "\n",
    "        # # Store the gradients\n",
    "        # self.hist_grad = hist\n",
    "\n",
    "        return hist\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Compute the gradients of the counts buffer\n",
    "        grad_input = grad_output * self.counts / self.counts.sum()\n",
    "\n",
    "        # # Reset the gradients\n",
    "        # self.hist_grad = torch.zeros(self.num_bins)\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "class SoftHistogram(nn.Module):\n",
    "    def __init__(self, bins, min, max, sigma=1):\n",
    "        super(SoftHistogram, self).__init__()\n",
    "        self.bins = bins\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        self.sigma = sigma\n",
    "        self.delta = float(max - min) / float(bins)\n",
    "        self.centers = float(min) + self.delta * (torch.arange(bins).float() + 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # a = torch.unsqueeze(x, 0)\n",
    "        # print(\"x.shape:\", x.shape, \"a.shape\", a.shape)\n",
    "        # b = torch.unsqueeze(self.centers, 1)\n",
    "        # print(\"centers.shape:\", self.centers.shape, \"b.shape\", b.shape)\n",
    "\n",
    "        \n",
    "        # x = torch.unsqueeze(x, 0) - torch.unsqueeze(self.centers, 1)\n",
    "        # x = torch.sigmoid(self.sigma * (x + self.delta/2)) - torch.sigmoid(self.sigma * (x - self.delta/2))\n",
    "        # x = x.sum(dim=1)\n",
    "        # return x\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        ## input should be reshaped into [B, len]\n",
    "        b, c, h, w = x.shape\n",
    "        input = x.view(B, -1)\n",
    "        x = torch.unsqueeze(input, 1) - torch.unsqueeze(self.centers, -1)\n",
    "        x = torch.sigmoid(self.sigma * x)\n",
    "        diff = torch.cat([torch.ones((b,1,h*w),device=input.device), x],dim=1) - torch.cat([x, torch.zeros((b,1,h*w),device=input.device)],dim=1)\n",
    "\n",
    "        diff = diff.sum(dim=-1)\n",
    "        diff[:,-2] += diff[:,-1]\n",
    "        return diff[:,:-1]\n",
    "\n",
    "#############################################\n",
    "# Differentiable Histogram Counting Method\n",
    "#############################################\n",
    "# https://github.com/hyk1996/pytorch-differentiable-histogram\n",
    "def differentiable_histogram(x, bins=255, min=0.0, max=1.0):\n",
    "\n",
    "    if len(x.shape) == 4:\n",
    "        n_samples, n_chns, _, _ = x.shape\n",
    "    elif len(x.shape) == 2:\n",
    "        n_samples, n_chns = 1, 1\n",
    "    else:\n",
    "        raise AssertionError('The dimension of input tensor should be 2 or 4.')\n",
    "\n",
    "    hist_torch = torch.zeros(n_samples, n_chns, bins).to(x.device)\n",
    "    delta = (max - min) / bins\n",
    "\n",
    "    BIN_Table = torch.arange(start=0, end=bins, step=1) * delta\n",
    "\n",
    "    for dim in range(1, bins-1, 1):\n",
    "        h_r = BIN_Table[dim].item()             # h_r\n",
    "        h_r_sub_1 = BIN_Table[dim - 1].item()   # h_(r-1)\n",
    "        h_r_plus_1 = BIN_Table[dim + 1].item()  # h_(r+1)\n",
    "\n",
    "        mask_sub = ((h_r > x) & (x >= h_r_sub_1)).float()\n",
    "        mask_plus = ((h_r_plus_1 > x) & (x >= h_r)).float()\n",
    "\n",
    "        hist_torch[:, :, dim] += torch.sum(((x - h_r_sub_1) * mask_sub).view(n_samples, n_chns, -1), dim=-1)\n",
    "        hist_torch[:, :, dim] += torch.sum(((h_r_plus_1 - x) * mask_plus).view(n_samples, n_chns, -1), dim=-1)\n",
    "\n",
    "    return hist_torch / delta\n",
    "\n",
    "# Bard\n",
    "# def differentiable_histogram_optimized(x, bins=255, min=0.0, max=1.0):\n",
    "#     if len(x.shape) == 4:\n",
    "#         n_samples, n_chns, _, _ = x.shape\n",
    "#     elif len(x.shape) == 2:\n",
    "#         n_samples, n_chns = 1, 1\n",
    "#     else:\n",
    "#         raise AssertionError('The dimension of input tensor should be 2 or 4.')\n",
    "\n",
    "#     hist_torch = torch.zeros(n_samples, n_chns, bins).to(x.device)\n",
    "#     delta = (max - min) / bins\n",
    "\n",
    "#     # BIN_Table = torch.arange(start=0, end=bins, step=1) * delta\n",
    "\n",
    "#     print(x.shape, BIN_Table.shape)\n",
    "\n",
    "#     # Create a boolean mask representing which bin each pixel belongs to\n",
    "#     mask = (x >= BIN_Table[:-1]).float() & (x < BIN_Table[1:])\n",
    "\n",
    "#     # Calculate the histogram using a single accumulation step\n",
    "#     hist_torch = torch.sum(mask.view(n_samples, n_chns, bins, 1) * BIN_Table, dim=3)\n",
    "\n",
    "#     return hist_torch / delta\n",
    "\n",
    "# Chat-GPT\n",
    "def differentiable_histogram_optimized(x, bins=255, min=0.0, max=1.0):\n",
    "    if len(x.shape) == 4:\n",
    "        n_samples, n_chns, _, _ = x.shape\n",
    "    elif len(x.shape) == 2:\n",
    "        n_samples, n_chns = 1, 1\n",
    "    else:\n",
    "        raise AssertionError('The dimension of input tensor should be 2 or 4.')\n",
    "\n",
    "    hist_torch = torch.zeros(n_samples, n_chns, bins).to(x.device)\n",
    "    delta = (max - min) / bins\n",
    "\n",
    "    bin_edges = torch.linspace(min, max, bins + 1, device=x.device)\n",
    "\n",
    "    # Compute bin indices for each element in the input tensor\n",
    "    bin_indices = torch.bucketize(x, bin_edges, right=True)\n",
    "\n",
    "    # Create masks for each bin\n",
    "    mask_sub = (bin_indices > 0).float()\n",
    "    mask_plus = (bin_indices < bins).float()\n",
    "\n",
    "    # Compute contributions to the histogram using cumulative sum\n",
    "\n",
    "    print(bin_indices, bin_indices.shape)\n",
    "    print(hist_torch.shape)\n",
    "    print(mask_plus.shape, mask_sub.shape)\n",
    "    print(torch.cumsum(x, dim=-1).shape)\n",
    "    print((torch.cumsum(x, dim=-1)[:, bin_indices] * mask_sub).shape)\n",
    "    # hist_torch += torch.cumsum(x, dim=-1)[:, :, bin_indices] * mask_sub\n",
    "    # hist_torch[:, :, 1:] -= torch.cumsum(x, dim=-1)[:, :, bin_indices - 1] * mask_sub\n",
    "    # hist_torch[:, :, :-1] += torch.cumsum(x, dim=-1)[:, :, bin_indices + 1] * mask_plus\n",
    "    # hist_torch[:, :, -1] -= torch.cumsum(x, dim=-1)[:, :, bin_indices] * mask_plus\n",
    "\n",
    "    hist_torch += torch.cumsum(x, dim=-1)[:, bin_indices] * mask_sub\n",
    "    hist_torch[:, :, 1:] -= torch.cumsum(x, dim=-1)[:, bin_indices - 1] * mask_sub\n",
    "    hist_torch[:, :, :-1] += torch.cumsum(x, dim=-1)[:, bin_indices + 1] * mask_plus\n",
    "    hist_torch[:, :, -1] -= torch.cumsum(x, dim=-1)[:, bin_indices] * mask_plus\n",
    "    \n",
    "\n",
    "    return hist_torch / delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Student's T Table)[https://www.craftonhills.edu/current-students/tutoring-center/mathematics-tutoring/distribution_tables_normal_studentt_chisquared.pdf]  \n",
    "$\\alpha = 0.8$, $DF = +\\inf$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashFunction(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layers_widths: List[int],\n",
    "        input_dim: int = 2,\n",
    "        output_dim: int = 1,\n",
    "        hash_table_size: int = 2**14,\n",
    "        sigma_scale: float = 1.0,\n",
    "        should_log: int = 0, \n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Hash function module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_layers_widths : List[int]\n",
    "            List of hidden layers widths.\n",
    "        input_dim : int, optional (default is 2)\n",
    "            Input dimension\n",
    "        output_dim : int, optional (default is 1)\n",
    "            Output dimension\n",
    "        hash_table_size : int, optional (default is 2**14)\n",
    "            Hash table size.\n",
    "        sigma_scale : float, optional (default is 1.0)\n",
    "            Sigma scale.\n",
    "        should_log : int, optional (default is 0)\n",
    "            - 0: No logging.\n",
    "            - > 0: Log forward pass.\n",
    "            - > 1: Log layers grads and outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        super(HashFunction, self).__init__()\n",
    "\n",
    "        self._hash_table_size: int = hash_table_size - 1\n",
    "        self._sigma_scale: float = sigma_scale\n",
    "\n",
    "        self._alpha: float = 0.8 # student's t-distribution confidence level\n",
    "        \n",
    "        self._should_log: int = should_log\n",
    "\n",
    "        layers_widths: List[int] = [input_dim, *hidden_layers_widths, output_dim]\n",
    "\n",
    "        self.module_list: nn.ModuleList = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=layers_widths[i], out_features=layers_widths[i + 1]),\n",
    "                nn.ReLU() if i < len(layers_widths) - 2 else nn.Sigmoid()\n",
    "            ) for i in range(len(layers_widths) - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        print2((\"Input:\", x, x.shape), (self._should_log > 0))\n",
    "        print2((\"Input grad:\", x.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        for i, layer in enumerate(self.module_list):\n",
    "            x = layer(x)\n",
    "\n",
    "        print2((f\"After layers:\", f\"Output: {x}, shape: {x.shape}\"), (self._should_log > 0))\n",
    "        print2((f\"After layers:\", f\"Grad info: {x.requires_grad}, {x.grad_fn}\"), (self._should_log > 1))\n",
    "        \n",
    "        # x = torch.nan_to_num(x) # Sanitize nan to 0.0\n",
    "\n",
    "        if x.shape[-1] == 1: # directly indices\n",
    "            indices = GaussianConvolution.apply(x, self._hash_table_size)\n",
    "            print2((\"GaussianConvolution:\", indices, indices.shape), (self._should_log > 0))\n",
    "            print2((\"GaussianConvolution grad:\", indices.requires_grad, ), (self._should_log > 1))\n",
    "\n",
    "            sigma = None\n",
    "        else: # mu and sigma            \n",
    "            x = x.unsqueeze(-1)\n",
    "            print2((\"x:\", x, x.shape), (self._should_log > 0))\n",
    "\n",
    "            sigma = (x[..., 1, :] * self._sigma_scale)\n",
    "            x = x[..., 0, :]\n",
    "            mu = (differentiable_round(x * self._hash_table_size))\n",
    "\n",
    "            print2((\"Mu:\", mu, mu.shape), (self._should_log > 0))\n",
    "            print2((\"Sigma:\", sigma, sigma.shape), (self._should_log > 0))\n",
    "\n",
    "            a, b = norm.interval(self._alpha, loc=mu.detach().cpu().numpy(), scale=sigma.detach().cpu().numpy()) # a -> at left of mean, b -> at right of mean\n",
    "            a, b = np.round(a), np.round(b)\n",
    "\n",
    "            print2((\"a:\", a, a.shape), (self._should_log > 0))\n",
    "            print2((\"b:\", b, b.shape), (self._should_log > 0))\n",
    "\n",
    "            indices = mu\n",
    "\n",
    "            del mu, a, b\n",
    "\n",
    "        return x, indices, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiresolution(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_min: int,\n",
    "        n_max: int,\n",
    "        num_levels: int,\n",
    "        HashFunction: HashFunction,\n",
    "        hash_table_size: int = 2**14,\n",
    "        input_dim: int = 2,\n",
    "        should_log: int = 0\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Multiresolution module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_min : int\n",
    "            Minimum scaling factor.\n",
    "        n_max : int\n",
    "            Maximum scaling factor.\n",
    "        num_levels : int\n",
    "            Number of levels.\n",
    "        HashFunction : HashFunction\n",
    "            Hash function module.\n",
    "        hash_table_size : int, optional (default is 2**14)\n",
    "            Hash table size.\n",
    "        input_dim : int, optional (default is 2)\n",
    "            Input dimension.\n",
    "        should_log : int, optional (default is 0)\n",
    "            - 0: No logging.\n",
    "            - > 0: Log forward pass.\n",
    "            - > 1: Log helper functions.\n",
    "            - > 2: Log collisions.\n",
    "            - > 3: Log dummies.\n",
    "            - > 5: Log initialization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        super(Multiresolution, self).__init__()\n",
    "\n",
    "        self.HashFunction: nn.Module = HashFunction\n",
    "\n",
    "        self._hash_table_size: int = hash_table_size\n",
    "        self._num_levels: int = num_levels\n",
    "        self._input_dim: int = input_dim\n",
    "        self._should_log: int = should_log\n",
    "\n",
    "        b: torch.Tensor = torch.tensor(np.exp((np.log(n_max) - np.log(n_min)) / (self._num_levels - 1))).float()\n",
    "        if b > 2 or b <= 1:\n",
    "            print(\n",
    "                f\"The between level scale is recommended to be <= 2 and needs to be > 1 but was {b:.4f}.\"\n",
    "            )\n",
    "        \n",
    "        self._levels: torch.Tensor = torch.stack([\n",
    "            torch.floor(n_min * (b ** l)) for l in range(self._num_levels)\n",
    "        ]).reshape(1, 1, -1, 1)\n",
    "        print2((\"Levels:\", self._levels, self._levels.shape), (self._should_log > 5))\n",
    "        print2((\"Levels grads:\", self._levels.requires_grad, ), (self._should_log > 5))\n",
    "\n",
    "        self._voxels_helper_hypercube: torch.Tensor = rearrange(\n",
    "            torch.tensor(\n",
    "                np.stack(np.meshgrid(range(2), range(2), range(self._input_dim - 1), indexing=\"ij\"), axis=-1)\n",
    "            ),\n",
    "            \"cols rows depths verts -> (depths rows cols) verts\"\n",
    "        ).T[:self._input_dim, :].unsqueeze(0).unsqueeze(2)\n",
    "        print2((\"voxels_helper_hypercube:\", self._voxels_helper_hypercube, self._voxels_helper_hypercube.shape), (self._should_log > 5))\n",
    "        print2((\"voxels_helper_hypercube grads:\", self._voxels_helper_hypercube.requires_grad), (self._should_log > 5))\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        should_calc_hists: bool = False\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        # print2((\"x:\", x, x.shape), (self._should_log > 0))\n",
    "        print2((\"x grads:\", x.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        scaled_coords, grid_coords = self._scale_to_grid(x)\n",
    "\n",
    "        probs, hashed, sigmas = self.HashFunction(grid_coords)\n",
    "        print2((\"hashed:\", hashed, hashed.shape), (self._should_log > 0))\n",
    "        print2((\"hashed grads:\", hashed.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        print2((\"probs:\", probs, probs.shape), (self._should_log > 0))\n",
    "        print2((\"probs grads:\", probs.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        print2((\"sigmas:\", sigmas, sigmas.shape), (self._should_log > 0))\n",
    "        print2((\"sigmas grads:\", sigmas.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        dummy_grids, dummy_hashed, og_indices = self._calc_dummies(grid_coords, hashed)\n",
    "\n",
    "        collisions, min_possible_collisions = self.calc_hash_collisions(dummy_grids, dummy_hashed)\n",
    "        del dummy_hashed\n",
    "\n",
    "        unique_probs, unique_hashed, unique_sigmas = self._calc_uniques(probs, hashed, sigmas, og_indices)\n",
    "        del og_indices\n",
    "\n",
    "        if should_calc_hists:\n",
    "            hists = self._hist_collisions(dummy_grids, unique_hashed, should_show=False)\n",
    "        else:\n",
    "            hists = None\n",
    "        del dummy_grids, sigmas\n",
    "\n",
    "        return hashed, unique_hashed, unique_probs, unique_sigmas, collisions, min_possible_collisions, hists\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _scale_to_grid(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Scale coordinates to grid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Original coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]\n",
    "            Scaled coordinates, grid coordinates and dummy grids.\n",
    "        \"\"\"\n",
    "\n",
    "        scaled_coords: torch.Tensor = x.float() * self._levels.float()\n",
    "        print2((\"scaled_coords:\", scaled_coords, scaled_coords.shape), (self._should_log > 1))\n",
    "        print2((\"scaled_coords grads:\", scaled_coords.requires_grad, ), (self._should_log > 1))\n",
    "\n",
    "        grid_coords: torch.Tensor = rearrange(\n",
    "            torch.add(\n",
    "                torch.floor(scaled_coords),\n",
    "                self._voxels_helper_hypercube\n",
    "            ),\n",
    "            \"batch pixels xyz levels verts -> batch pixels levels verts xyz\"\n",
    "        )\n",
    "        print2((\"grid_coords:\", grid_coords, grid_coords.shape), (self._should_log > 1))\n",
    "        print2((\"grid_coords grads:\", grid_coords.requires_grad, ), (self._should_log > 1))\n",
    "\n",
    "        return scaled_coords, grid_coords\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _calc_dummies(\n",
    "        self,\n",
    "        grid_coords: torch.Tensor,\n",
    "        hashed: torch.Tensor,\n",
    "    ) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Calculate dummies.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid_coords : torch.Tensor\n",
    "            Grid coordinates.\n",
    "        hashed : torch.Tensor\n",
    "            Hashed grid coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]]\n",
    "            Dummy grids and dummy hashed and original unique indices.\n",
    "        \"\"\"\n",
    "\n",
    "        dummy_grids: List[Tuple[torch.Tensor]] = [\n",
    "            # [\n",
    "            torch.unique(rearrange(grid_coords[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\"), dim=0, return_inverse=True) # first dimension is 0 because images should have all same size\n",
    "            for l in range(self._num_levels)\n",
    "            # ] for b in range(grid_coords.shape[0])\n",
    "        ]\n",
    "\n",
    "        dummy_grids, dummy_grids_inverse_indices = zip(*dummy_grids)\n",
    "        print2((\"dummy_grids:\", dummy_grids, [dummy_grids[l].shape for l in range(self._num_levels)]), (self._should_log > 1))\n",
    "        print2((\"dummy_grids grads:\", [(dummy_grids[l].requires_grad, ) for l in range(self._num_levels)]), (self._should_log > 3))\n",
    "\n",
    "        og_indices = [\n",
    "            torch.tensor([np.where(dummy_grids_inverse_indices[l].detach().cpu().numpy() == i)[0][0] for i in range(len(dummy_grids[l]))])\n",
    "            for l in range(self._num_levels)\n",
    "        ]\n",
    "        print2((\"og_indices:\", [(og_indices[l], og_indices[l].shape) for l in range(self._num_levels)]), (self._should_log > 1))\n",
    "        print2((\"og_indices grads:\", [(og_indices[l].requires_grad, ) for l in range(self._num_levels)]), (self._should_log > 1))\n",
    "\n",
    "        dummy_hashed: List[torch.Tensor] = [\n",
    "            # [\n",
    "            torch.unique(rearrange(hashed[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\"), dim=0, return_inverse=False) # first dimension is 0 because images should have all same size\n",
    "            for l in range(self._num_levels)\n",
    "            # ] for b in range(hashed.shape[0])\n",
    "        ]\n",
    "        print2((\"dummy_hashed:\", dummy_hashed, [dummy_hashed[l].shape for l in range(self._num_levels)]), (self._should_log > 3))\n",
    "        print2((\"dummy_hashed grads:\", [(dummy_hashed[l].requires_grad, ) for l in range(self._num_levels)]), (self._should_log > 3))\n",
    "\n",
    "        return dummy_grids, dummy_hashed, og_indices\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calc_hash_collisions(\n",
    "        self, \n",
    "        dummy_grids: List[torch.Tensor], \n",
    "        dummy_hashed: List[torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate hash collisions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dummy_grid_coords : List[torch.Tensor]\n",
    "            Grid coordinates.\n",
    "        dummy_hashed : List[torch.Tensor]\n",
    "            Hashed grid coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "            Collisions and minimum possible collisions at each level.\n",
    "        \"\"\"\n",
    "\n",
    "        min_possible_collisions: torch.Tensor = torch.stack([\n",
    "            torch.tensor((dummy_grids[l].shape[0]) - self._hash_table_size)\n",
    "            for l in range(self._num_levels)\n",
    "        ])\n",
    "        min_possible_collisions[min_possible_collisions < 0] = 0\n",
    "        print2((\"min_possible_collisions:\", min_possible_collisions, min_possible_collisions.shape), (self._should_log > 2))\n",
    "        print2((\"min_possible_collisions grads:\", min_possible_collisions.requires_grad), (self._should_log > 2))\n",
    "\n",
    "        collisions: torch.Tensor = torch.stack([\n",
    "            # [\n",
    "            torch.tensor(float(dummy_grids[l].shape[0] - torch.unique(dummy_hashed[l], dim=0).shape[0]))#, requires_grad=True)\n",
    "            for l in range(self._num_levels)\n",
    "            # ] \n",
    "            # for b in range(hashed.shape[0])\n",
    "        ])\n",
    "        print2((\"collisions:\", collisions, collisions.shape, collisions.dtype), (self._should_log > 2))\n",
    "        print2((\"collisions grads:\", collisions.requires_grad, ), (self._should_log > 2))\n",
    "\n",
    "        return collisions, min_possible_collisions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _calc_uniques(\n",
    "        self,\n",
    "        probs: torch.Tensor,\n",
    "        hashed: torch.Tensor,\n",
    "        sigmas: torch.Tensor,\n",
    "        og_indices: List[torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate uniques.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probs : torch.Tensor\n",
    "            Probabilities.\n",
    "        hashed : torch.Tensor\n",
    "            Hashed grid coordinates.\n",
    "        sigmas : torch.Tensor\n",
    "            Sigmas of hashed grid coordinates.\n",
    "        og_indices : List[torch.Tensor]\n",
    "            Original unique indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor]\n",
    "            Unique probabilities, unique hashed, and unique_sigmas.\n",
    "        \"\"\"\n",
    "\n",
    "        unique_probs: List[torch.Tensor] = [\n",
    "            # [\n",
    "            F.softmax(rearrange(probs[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\")[og_indices[l]], dim=0)\n",
    "            for l in range(self._num_levels)\n",
    "            # ]\n",
    "            # for b in range(probs.shape[0])\n",
    "        ]\n",
    "        print2((\"unique_probs:\", [(unique_probs[b][l], unique_probs[b][l].shape) for l in range(self._num_levels) for b in range(probs.shape[0])]), (self._should_log > 0))\n",
    "        print2((\"unique_probs:\", [(unique_probs[l], unique_probs[l].shape) for l in range(self._num_levels)]), (self._should_log > 0))\n",
    "\n",
    "        unique_hashed: List[torch.Tensor] = [\n",
    "            # [\n",
    "            rearrange(hashed[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\")[og_indices[l]]\n",
    "            for l in range(self._num_levels)\n",
    "            # ]\n",
    "            # for b in range(probs.shape[0])\n",
    "        ]\n",
    "        print2((\"unique_hashed:\", [(unique_hashed[b][l], unique_hashed[b][l].shape) for l in range(self._num_levels) for b in range(probs.shape[0])]), (self._should_log > 0))\n",
    "        print2((\"unique_hashed:\", [(unique_hashed[l], unique_hashed[l].shape) for l in range(self._num_levels)]), (self._should_log > 0))\n",
    "\n",
    "        unique_sigmas: List[torch.Tensor] = [\n",
    "            # [\n",
    "            rearrange(sigmas[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\")[og_indices[l]]\n",
    "            for l in range(self._num_levels)\n",
    "            # ]\n",
    "            # for b in range(probs.shape[0])\n",
    "        ]\n",
    "        print2((\"unique_sigmas:\", [(unique_sigmas[b][l], unique_sigmas[b][l].shape) for l in range(self._num_levels) for b in range(probs.shape[0])]), (self._should_log > 0))\n",
    "        print2((\"unique_sigmas:\", [(unique_sigmas[l], unique_sigmas[l].shape) for l in range(self._num_levels)]), (self._should_log > 0))\n",
    "\n",
    "\n",
    "        return unique_probs, unique_hashed, unique_sigmas\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _hist_collisions(\n",
    "        self,\n",
    "        dummy_grids: List[torch.Tensor],\n",
    "        dummy_hashed: List[torch.Tensor],\n",
    "        should_show: bool = False\n",
    "    ) -> List[plt.Figure]:\n",
    "        \"\"\"\n",
    "        Show collisions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dummy_grids : List[torch.Tensor]\n",
    "            Grid coordinates.\n",
    "        dummy_hashed : List[torch.Tensor]\n",
    "            Hashed grid coordinates.\n",
    "        should_show : bool, optional (default is False)\n",
    "            Whether to show figure.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[plt.Figure]\n",
    "            Histograms of collisions, one per level.\n",
    "        \"\"\"\n",
    "\n",
    "        figs=[]\n",
    "\n",
    "        for l in range(self._num_levels):\n",
    "\n",
    "            indices = dummy_hashed[l].detach().cpu().numpy()\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(15, 5))\n",
    "            ax.hist(\n",
    "                indices,\n",
    "                bins=self._hash_table_size,\n",
    "                range=(0, self._hash_table_size),\n",
    "                edgecolor='grey', \n",
    "                linewidth=0.5\n",
    "            )\n",
    "\n",
    "            ax.set_xlim(-1, self._hash_table_size)\n",
    "            ax.xaxis.set_ticks(np.arange(0, self._hash_table_size, 10))\n",
    "\n",
    "            start, end = ax.get_ylim()\n",
    "            step = int(end * 0.1)\n",
    "            ax.yaxis.set_ticks(np.arange(0, end, step if step > 0 else 1))\n",
    "\n",
    "            plt.title(f\"Level {l} ({int(self._levels[0, 0, l, 0].item())})\")\n",
    "            plt.xlabel(\"Hashed indices\")\n",
    "            plt.ylabel(\"Counts\")\n",
    "\n",
    "            figs.append(fig)\n",
    "\n",
    "            if should_show:\n",
    "                plt.show()\n",
    "            \n",
    "            plt.close()\n",
    "\n",
    "        return figs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    x: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    should_calc_hists: bool = False,\n",
    "    should_kl_hist: bool = False,\n",
    "    should_log: int = 0,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[plt.Figure], torch.Tensor, torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Train loop.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        Train input data.\n",
    "    model : nn.Module\n",
    "        Model to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer.\n",
    "    loss_fn : nn.Module\n",
    "        Loss function.\n",
    "    should_calc_hists : bool, optional (default is False)\n",
    "        Whether to calculate histograms or not.\n",
    "    should_kl_hist : bool, optional (default is False)\n",
    "        Whether to calculate KL with histograms or not.\n",
    "    should_log : int, optional (default is 0)\n",
    "        - 0: No logging.\n",
    "        - > 0: Log\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[plt.Figure], torch.Tensor, torch.Tensor, float]\n",
    "        Model output, unique probabilities, collisions, histograms, collisions losses, KL losses and loss.\n",
    "    \"\"\"\n",
    "    print2((\"Train loop\", ), should_log > 0, bcolors.WARNING)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out, unique_hashed, unique_probs, unique_sigmas, collisions, min_possible_collisions, hists = multires(\n",
    "        x, \n",
    "        should_calc_hists=should_calc_hists \n",
    "    )\n",
    "\n",
    "    # print(\"out shape:\", out.shape)\n",
    "\n",
    "    # print(f\"Unique hashed: {[unique_hashed[l].shape for l in range(len(unique_hashed))]}\")\n",
    "\n",
    "    # print(f\"Softmax Unique hashed: {torch.softmax(unique_hashed[0].squeeze(-1), dim=-1), unique_hashed[0].shape}\")\n",
    "\n",
    "    # print(f\"Unique probs: {unique_probs[0], unique_probs[0].shape}\")\n",
    "\n",
    "    collisions_losses, kl_losses, sigma_loss, loss = loss_fn(\n",
    "        collisions, \n",
    "        min_possible_collisions, \n",
    "        unique_probs if not should_kl_hist else rearrange(out, \"batch pixels levels verts xyz -> batch levels pixels (verts xyz)\")[0], #unique_hashed,  \n",
    "        unique_sigmas\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return out, unique_probs, collisions, min_possible_collisions, hists, collisions_losses, kl_losses, sigma_loss, loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(\n",
    "    x: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    loss_fn: nn.Module,\n",
    "    should_calc_hists: bool = False,\n",
    "    should_kl_hist: bool = False,\n",
    "    should_log: int = 0,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[plt.Figure], torch.Tensor, torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Test loop.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        Test input data.\n",
    "    model : nn.Module\n",
    "        Model to test.\n",
    "    loss_fn : nn.Module\n",
    "        Loss function.\n",
    "    should_calc_hists : bool, optional (default is False)\n",
    "        Whether to calculate histograms or not.\n",
    "    should_kl_hist : bool, optional (default is False)\n",
    "        Whether to calculate KL with histograms or not.\n",
    "    should_log : int, optional (default is 0)\n",
    "        - 0: No logging.\n",
    "        - > 0: Log\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[plt.Figure], torch.Tensor, torch.Tensor, float]\n",
    "        Model output, unique probabilities, collisions, histograms, collisions losses, KL losses and loss.\n",
    "    \"\"\"\n",
    "    print2((\"Test loop\", ), should_log > 0, bcolors.FAIL)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    out, unique_hashed, unique_probs, unique_sigmas, collisions, min_possible_collisions, hists = multires(\n",
    "        x, \n",
    "        should_calc_hists=should_calc_hists \n",
    "    )\n",
    "\n",
    "    collisions_losses, kl_losses, sigma_loss, loss = loss_fn(\n",
    "        collisions, \n",
    "        min_possible_collisions, \n",
    "        unique_probs if not should_kl_hist else rearrange(out, \"batch pixels levels verts xyz -> batch levels pixels (verts xyz)\")[0], #unique_hashed, \n",
    "        unique_sigmas\n",
    "    )\n",
    "\n",
    "    return out, unique_probs, collisions, min_possible_collisions, hists, collisions_losses, kl_losses, sigma_loss, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics, Loss & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        l_collisions: float,\n",
    "        l_kl_loss: float,\n",
    "        l_sigma_loss: float,\n",
    "        delta: float = 1,\n",
    "        hash_table_size: int = 2**14,\n",
    "        should_kl_hist: bool = False,\n",
    "        should_log: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Loss module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        min_possible_collisions : torch.Tensor\n",
    "            Minimum possible collisions per level.\n",
    "        l_collisions : float\n",
    "            Collisions loss lambda.\n",
    "        l_kl_loss : float\n",
    "            KL loss lambda.\n",
    "        l_sigma_loss : float\n",
    "            Sigma loss lambda.\n",
    "        delta : float, optional (default is 1)\n",
    "            Delta parameter for collision loss.\n",
    "        hash_table_size : int, optional (default is 2**14)\n",
    "            Hash table size.\n",
    "        should_kl_hist : bool, optional (default is False)\n",
    "            Whether to calculate KL with histograms or not.\n",
    "        should_log : int, optional (default is 0)\n",
    "            - 0: No logging.\n",
    "            - > 0: Log forward pass.\n",
    "            - > 1: Log helper functions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "        self._l_collisions = l_collisions\n",
    "        self._l_kl_loss = l_kl_loss\n",
    "        self._l_sigma_loss = l_sigma_loss\n",
    "\n",
    "        self._delta = delta\n",
    "        self._hash_table_size = hash_table_size\n",
    "        self._should_kl_hist = should_kl_hist\n",
    "        self._should_log = should_log\n",
    "\n",
    "        self.kl_div_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "        # self.differentiable_hist = DifferentiableHistogram(self._hash_table_size)\n",
    "        self.differentiable_hist = SoftHistogram(bins=self._hash_table_size, min=0, max=self._hash_table_size, sigma=1.0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        collisions: List,\n",
    "        min_possible_collisions: List,\n",
    "        probs: List,\n",
    "        sigmas: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        # min_possible_collisions[min_possible_collisions <= 0] = self._delta\n",
    "        delta = min_possible_collisions.clone()\n",
    "        delta[delta <= 0] = self._delta\n",
    "    \n",
    "        collisions_losses: torch.Tensor = (collisions - min_possible_collisions) / delta # min_possible_collisions\n",
    "        print2((\"Collisions Losses:\", collisions_losses), self._should_log > 0)\n",
    "        print2((\"Collisions Losses grads:\", collisions_losses.requires_grad), self._should_log > 0)\n",
    "\n",
    "        kl_div_losses = torch.stack([\n",
    "            (\n",
    "                self._kl_div(prob.shape[0], prob.squeeze(-1)) \n",
    "                if not self._should_kl_hist \n",
    "                else self._kl_div(0, prob)\n",
    "            )\n",
    "            for prob in probs\n",
    "        ])\n",
    "\n",
    "        print2((\"KL Losses:\", kl_div_losses), self._should_log > 0, bcolors.FAIL)\n",
    "\n",
    "        sigma_losses = torch.stack([\n",
    "            self.mse_loss(sigma, torch.zeros_like(sigma))\n",
    "            for sigma in sigmas\n",
    "        ])\n",
    "\n",
    "        collisions_loss = torch.sum(collisions_losses)\n",
    "        kl_loss = kl_div_losses.sum()\n",
    "        sigma_loss = sigma_losses.sum()\n",
    "        \n",
    "        loss = (self._l_collisions * collisions_loss) + (self._l_kl_loss * kl_loss) + (self._l_sigma_loss * sigma_loss)\n",
    "        # loss = abs(loss)\n",
    "        print2((\"Loss grads:\", loss.requires_grad, ), should_log > 0, bcolors.HEADER)\n",
    "\n",
    "        return collisions_losses, kl_div_losses, sigma_losses, loss\n",
    "\n",
    "    # def hist_c_differentiable(self, data, bins):\n",
    "    #     pooled_data, _ = torch.max(data, dim=-1)\n",
    "    #     print(pooled_data)\n",
    "    #     hist = torch.histc(pooled_data, bins=bins)\n",
    "    #     return hist\n",
    "\n",
    "    def _kl_div(\n",
    "        self,\n",
    "        level: int,\n",
    "        p: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate KL divergence loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        level : int\n",
    "            Level.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            KL divergence loss.\n",
    "        \"\"\"\n",
    "\n",
    "        print2((f\"p: {p}, shape: {p.shape}, requires_grad: {p.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "        if self._should_kl_hist:\n",
    "            hist_p_nondiff = torch.histc(p, bins=self._hash_table_size, min=0, max=self._hash_table_size) # ? maybe max=(self._hash_table_size - 1)\n",
    "            print2((f\"hist_p_nondiff: {hist_p_nondiff}, shape: {hist_p_nondiff.shape}, sum: {torch.sum(hist_p_nondiff)}, requires_grad: {hist_p_nondiff.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "            # Other differentiable histogram methods\n",
    "            # hist_p = self.hist_c_differentiable(p, bins=self._hash_table_size)\n",
    "            # hist_p = self.differentiable_hist(p)\n",
    "            # hist_p = differentiable_histogram_optimized(p, bins=self._hash_table_size, min=0, max=self._hash_table_size).squeeze(0).squeeze(0)\n",
    "\n",
    "            hist_p = differentiable_histogram(p, bins=self._hash_table_size, min=0, max=self._hash_table_size).squeeze(0).squeeze(0)\n",
    "            print2((f\"hist_p: {hist_p}, shape: {hist_p.shape}, sum: {torch.sum(hist_p)}, requires_grad: {hist_p.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "            print(\"Count(hist_p_nondiff != hist_p):\", hist_p_nondiff.shape[0] - torch.sum(torch.eq(hist_p_nondiff, hist_p)))\n",
    "\n",
    "            p = hist_p / self._hash_table_size\n",
    "            print2((f\"p: {p}, shape: {p.shape}, requires_grad: {p.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "            p[p == 0] = 1e-10\n",
    "            print2((f\"after p: {p}, shape: {p.shape}, requires_grad: {p.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "            q = torch.ones(self._hash_table_size) / self._hash_table_size\n",
    "        else:\n",
    "            # q = torch.ones(level) / level\n",
    "            q = torch.arange(level, dtype=torch.float32)\n",
    "            print2((f\"before q: {q}, shape: {q.shape}, requires_grad: {q.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "            q = torch.softmax(q, dim=-1)\n",
    "\n",
    "        print2((f\"after q: {q}, shape: {q.shape}, requires_grad: {q.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "        return self.kl_div_loss(p.log(), q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(\n",
    "    net: torch.nn.Module,\n",
    "    # encoding_lr: float,\n",
    "    hash_lr: float,\n",
    "    # MLP_lr: float,\n",
    "    # encoding_weight_decay: float,\n",
    "    hash_weight_decay: float,\n",
    "    # MLP_weight_decay: float,\n",
    "    betas: tuple = (0.9, 0.99),\n",
    "    eps: float = 1e-15\n",
    "):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            # {\"params\": net.encoding.parameters(), \"lr\": encoding_lr, \"weight_decay\": encoding_weight_decay},\n",
    "            {\"params\": net.HashFunction.parameters(), \"lr\": hash_lr, \"weight_decay\": hash_weight_decay},\n",
    "            # {\"params\": net.mlp.parameters(), \"lr\": MLP_lr, \"weight_decay\": MLP_weight_decay}\n",
    "        ],\n",
    "        betas=betas,\n",
    "        eps=eps,\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, tolerance: int = 5, min_delta: int = 0, should_reset: bool = True):\n",
    "        self.tolerance: int = tolerance\n",
    "        self.min_delta: int = min_delta\n",
    "        self.best_loss: float = np.inf\n",
    "        self.counter: int = 0\n",
    "        self.early_stop: bool = False\n",
    "        self._should_reset: bool = should_reset\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        # print(f\"best_loss: {self.best_loss}, loss: {loss}, counter: {self.counter}\")\n",
    "\n",
    "        if abs(self.best_loss - loss) < self.min_delta and (loss < self.best_loss):\n",
    "            # print(\"Stall\")\n",
    "            self.counter += 1\n",
    "        elif abs(self.best_loss - loss) > self.min_delta and (loss > self.best_loss):\n",
    "            # print(\"Growing\")\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            if not self._should_reset:\n",
    "                if self.counter <= 0:\n",
    "                    self.counter = 0\n",
    "                else:\n",
    "                    self.counter -= 1\n",
    "            else:\n",
    "                self.counter = 0\n",
    "                self.best_loss = loss\n",
    "\n",
    "        if self.counter >= self.tolerance:\n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: 20231207164646\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./images\"\n",
    "test_size = 0.2\n",
    "\n",
    "train_images, test_images = train_test_split(\n",
    "    [\n",
    "        file for file in os.listdir(root_dir) if (\"silhouette\" in file) and (file.endswith(\".jpg\") or file.endswith(\".png\") or file.endswith(\".jpeg\"))\n",
    "    ], \n",
    "    test_size=test_size, \n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "wandb_entity = \"fedemonti00\"\n",
    "wandb_project = \"project_course\"\n",
    "# wandb_name = \"hash_function_training\"\n",
    "\n",
    "try:\n",
    "    time = wandb_name\n",
    "except NameError:\n",
    "    time = (datetime.now(ZoneInfo(\"Europe/Rome\"))).strftime(\"%Y%m%d%H%M%S\")\n",
    "print(\"RUN:\", time)\n",
    "\n",
    "early_stopper_tolerance = 500\n",
    "early_stopper_min_delta = 1e-6 \n",
    "\n",
    "histogram_rate = 10\n",
    "\n",
    "hyperparameters = {\n",
    "    \"hash_table_size\": 2**8,\n",
    "    \"n_min\": 8,\n",
    "    \"n_max\": 32,\n",
    "    \"num_levels\": 4,\n",
    "    \"output_dim\": 2,\n",
    "    \"hash_function_hidden_layers_widths\": [8, 32, 8],\n",
    "    # \"hash_lr\": 1e-3,\n",
    "    \"hash_lr\": 1e-4,\n",
    "    \"hash_weight_decay\": 1e-6,\n",
    "    \"kl_hist_loss\": True,\n",
    "    # \"l_collisions\": 1e-1,\n",
    "    \"l_collisions\": 0,\n",
    "    # \"l_kl_loss\": -100,\n",
    "    # \"l_kl_loss\": 100,\n",
    "    \"l_kl_loss\": 1,\n",
    "    # \"l_sigma_loss\": 1e-1,\n",
    "    \"l_sigma_loss\": 0,\n",
    "    # \"epochs\": 1000,\n",
    "    \"epochs\": 1,\n",
    "    \"random_seed\": random_seed,\n",
    "}\n",
    "\n",
    "# should_log = False\n",
    "should_log = True\n",
    "should_wandb = True if hyperparameters[\"epochs\"] > 999 else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_wandb:\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        entity = wandb_entity,\n",
    "        # set the wandb project where this run will be logged\n",
    "        project = wandb_project,\n",
    "\n",
    "        name = time,\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config = hyperparameters,\n",
    "        # config = {\n",
    "        #     \"id_grid_search_params\":        id_param,\n",
    "        #     \"grid_search_params\":           params,\n",
    "        #     \"random_seed\":                  random_seed,\n",
    "        #     \"HPD_learning_rate\":            HPD_lr,\n",
    "        #     \"encoding_learning_rate\":       encoding_lr,\n",
    "        #     \"MLP_learning_rate\":            MLP_lr,\n",
    "        #     \"encoding_weight_decay\":        encoding_weight_decay,\n",
    "        #     \"HPD_weight_decay\":             HPD_weight_decay,\n",
    "        #     \"MLP_weight_decay\":             MLP_weight_decay,\n",
    "        #     \"batch_size%\":                  batch_size,\n",
    "        #     \"shuffled_pixels\":              should_shuffle_pixels,\n",
    "        #     \"normalized_data\":              True if not should_batchnorm_data else \"BatchNorm1d\",\n",
    "        #     \"architecture\":                 \"GeneralNeuralGaugeFields\",\n",
    "        #     \"dataset\":                      image_name,\n",
    "        #     \"epochs\":                       epochs,\n",
    "        #     \"color\":                        'RGB' if not should_bw else 'BW',\n",
    "        #     \"hash_table_size\":              hash_table_size,\n",
    "        #     \"num_levels\":                   num_levels,\n",
    "        #     \"n_min\":                        n_min,\n",
    "        #     \"n_max\":                        n_max,\n",
    "        #     \"MLP_hidden_layers_widths\":     str(MLP_hidden_layers_widths),\n",
    "        #     \"HPD_hidden_layers_widths\":     str(HPD_hidden_layers_widths),\n",
    "        #     \"HPD_out_features\":             HPD_out_features,\n",
    "        #     \"feature_dim\":                  feature_dim,\n",
    "        #     \"topk_k\":                       topk_k,\n",
    "        #     \"loss_type\":                    \"JS+KLDiv\" if should_sum_js_kl_div else (\"KLDiv\" if not should_js_div else \"JSDiv\"),\n",
    "        #     \"loss_lambda_MSE\":              l_mse,\n",
    "        #     \"loss_lambda_JS_KL\":            l_js_kl,\n",
    "        #     \"loss_lambda_collisions\":       l_collisions,\n",
    "        #     \"loss_gamma\":                   loss_gamma,\n",
    "        #     \"loss_epsilon\":                 loss_epsilon,\n",
    "        #     \"inplace_scatter\":              should_inplace_scatter,\n",
    "        #     \"MLP_activations\":              \"LeakyReLU\" if should_leaky_relu else \"ReLU\",\n",
    "        #     \"collisions_loss_probs\":        \"topk_only\" if should_keep_topk_only else \"hash_table_size\",\n",
    "        #     \"avg_topk_features\":            \"softmax_avg\" if should_softmax_topk_features else (\"weighted_avg\" if should_softmax_topk_features != None else None),\n",
    "        #     \"hash_type\":                    \"HPD\" if not should_use_hash_function else \"hash_function\"\n",
    "        # }\n",
    "\n",
    "        save_code = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_log(\n",
    "    e: int,\n",
    "    train_loss: float,\n",
    "    train_collisions: torch.Tensor,\n",
    "    train_min_possible_collisions: torch.Tensor,\n",
    "    train_collisions_losses: torch.Tensor,\n",
    "    train_kl_losses: torch.Tensor,\n",
    "    train_sigma_losses: torch.Tensor,\n",
    "    train_hists: List[plt.Figure],\n",
    "    test_loss: float,\n",
    "    test_collisions: torch.Tensor,\n",
    "    test_min_possible_collisions: torch.Tensor,\n",
    "    test_collisions_losses: torch.Tensor,\n",
    "    test_kl_losses: torch.Tensor,\n",
    "    test_sigma_losses: torch.Tensor,\n",
    "    test_hists: List[plt.Figure],\n",
    "    should_log_hists: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Log to wandb.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    e : int\n",
    "        Epoch.\n",
    "    train_loss : float\n",
    "        Train loss.\n",
    "    train_collisions : torch.Tensor\n",
    "        Train collisions.\n",
    "    train_min_possible_collisions : torch.Tensor\n",
    "        Train minimum possible collisions.\n",
    "    train_collisions_losses : torch.Tensor\n",
    "        Train collisions losses.\n",
    "    train_kl_losses : torch.Tensor\n",
    "        Train KL losses.\n",
    "    train_sigma_losses : torch.Tensor\n",
    "        Train sigma losses.\n",
    "    train_hists : List[plt.Figure]\n",
    "        Train histograms.\n",
    "    test_loss : float\n",
    "        Test loss.\n",
    "    test_collisions : torch.Tensor\n",
    "        Test collisions.\n",
    "    test_min_possible_collisions : torch.Tensor\n",
    "        Test minimum possible collisions.\n",
    "    test_collisions_losses : torch.Tensor\n",
    "        Test collisions losses.\n",
    "    test_kl_losses : torch.Tensor\n",
    "        Test KL losses.\n",
    "    test_sigma_losses : torch.Tensor\n",
    "        Test sigma losses.\n",
    "    test_hists : List[plt.Figure]\n",
    "        Test histograms.\n",
    "    should_log_hists : bool, optional (default is False)\n",
    "        Whether to log histograms or not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    log = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"test_loss\": test_loss,\n",
    "        # \"train_sigma_loss\": train_sigma_losses.item(),\n",
    "        # \"test_sigma_loss\": test_sigma_losses.item(),\n",
    "    }\n",
    "\n",
    "    for l in range(hyperparameters[\"num_levels\"]):\n",
    "        log[f\"train_collisions_level_{l}\"] = train_collisions[l].item()\n",
    "        log[f\"train_min_possible_collisions_level_{l}\"] = train_min_possible_collisions[l].item()\n",
    "        log[f\"train_collisions_loss_level_{l}\"] = train_collisions_losses[l].item()\n",
    "        log[f\"train_kl_loss_level_{l}\"] = train_kl_losses[l].item()\n",
    "        log[f\"train_sigma_loss_level_{l}\"] = train_sigma_losses[l].item()\n",
    "\n",
    "        log[f\"test_collisions_level_{l}\"] = test_collisions[l].item()\n",
    "        log[f\"test_min_possible_collisions_level_{l}\"] = test_min_possible_collisions[l].item()\n",
    "        log[f\"test_collisions_loss_level_{l}\"] = test_collisions_losses[l].item()\n",
    "        log[f\"test_kl_loss_level_{l}\"] = test_kl_losses[l].item()\n",
    "        log[f\"test_sigma_loss_level_{l}\"] = test_sigma_losses[l].item()\n",
    "        \n",
    "        if should_calc_hists:\n",
    "            log[f\"train_hist_counts_level_{l}\"] = wandb.Image(\n",
    "                train_hists[l],\n",
    "                caption=f\"Hashed indices counts at level {l} at epoch {e}\"\n",
    "            )\n",
    "\n",
    "            log[f\"test_hist_counts_level_{l}\"] = wandb.Image(\n",
    "                test_hists[l],\n",
    "                caption=f\"Hashed indices counts at level {l} at epoch {e}\"\n",
    "            )\n",
    "    \n",
    "    wandb.log(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImagesDataset(\n",
    "    root=root_dir.split(\"/\")[0],\n",
    "    dir_name=root_dir.split(\"/\")[1],\n",
    "    images_names=train_images\n",
    ")\n",
    "x, y, h, w, names = train_dataset[-1]\n",
    "\n",
    "test_dataset = ImagesDataset(\n",
    "    root=root_dir.split(\"/\")[0],\n",
    "    dir_name=root_dir.split(\"/\")[1],\n",
    "    images_names=[\"strawberry_small.jpg\"]#test_images\n",
    ")\n",
    "eval_x, eval_y, eval_h, eval_w, eval_names = test_dataset[-1]\n",
    "\n",
    "input_dim = x.shape[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiresolution(\n",
      "  (HashFunction): HashFunction(\n",
      "    (module_list): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=8, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=8, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=2, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hashFunction = HashFunction(\n",
    "    hidden_layers_widths=hyperparameters[\"hash_function_hidden_layers_widths\"],\n",
    "    input_dim=input_dim,\n",
    "    output_dim=hyperparameters[\"output_dim\"],\n",
    "    hash_table_size=hyperparameters[\"hash_table_size\"],\n",
    "    should_log=0 if should_log else 0\n",
    ")\n",
    "\n",
    "multires = Multiresolution(\n",
    "    n_min=hyperparameters[\"n_min\"],\n",
    "    n_max=hyperparameters[\"n_max\"],\n",
    "    num_levels=hyperparameters[\"num_levels\"],\n",
    "    HashFunction=hashFunction,\n",
    "    hash_table_size=hyperparameters[\"hash_table_size\"],\n",
    "    input_dim=input_dim,\n",
    "    should_log=0 if should_log else 0\n",
    ")\n",
    "\n",
    "loss_fn = Loss(\n",
    "    l_collisions=hyperparameters[\"l_collisions\"],\n",
    "    l_kl_loss=hyperparameters[\"l_kl_loss\"],\n",
    "    l_sigma_loss=hyperparameters[\"l_sigma_loss\"],\n",
    "    hash_table_size=hyperparameters[\"hash_table_size\"],\n",
    "    should_kl_hist=hyperparameters[\"kl_hist_loss\"],\n",
    "    should_log=2 if should_log else 0\n",
    ")\n",
    "\n",
    "optimizer = get_optimizer(\n",
    "    net=multires,\n",
    "    hash_lr=hyperparameters[\"hash_lr\"],\n",
    "    hash_weight_decay=hyperparameters[\"hash_weight_decay\"],\n",
    ")\n",
    "\n",
    "early_stopper = EarlyStopper(\n",
    "    tolerance=early_stopper_tolerance,\n",
    "    min_delta=early_stopper_min_delta\n",
    ")\n",
    "\n",
    "print(multires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m Line:  print2((\"Train loop\", ), should_log > 0, bcolors.WARNING) \u001b[0m\n",
      "Train loop\n",
      "\u001b[93m -------------------- \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/federicomontagna/collision_handling_in_instantNGP/project_course/lib/python3.10/site-packages/torch/utils/_device.py:77: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at ../aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  return func(*args, **kwargs)\n",
      "  0%|          | 0/1 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Line:  print2((\"Collisions Losses:\", collisions_losses), self._should_log > 0) \u001b[0m\n",
      "Collisions Losses:\n",
      "tensor([ 77.0000, 164.0000,   1.3405,   0.2917], device='cuda:0')\n",
      "\u001b[96m -------------------- \u001b[0m\n",
      "\u001b[96m Line:  print2((\"Collisions Losses grads:\", collisions_losses.requires_grad), self._should_log > 0) \u001b[0m\n",
      "Collisions Losses grads:\n",
      "False\n",
      "\u001b[96m -------------------- \u001b[0m\n",
      "\u001b[96m Line:  print2((f\"p: {p}, shape: {p.shape}, requires_grad: {p.requires_grad}\", ), self._should_log > 1) \u001b[0m\n",
      "p: tensor([[131., 131., 131., 131.],\n",
      "        [131., 131., 131., 131.],\n",
      "        [131., 131., 131., 131.],\n",
      "        ...,\n",
      "        [130., 130., 129., 129.],\n",
      "        [130., 130., 129., 129.],\n",
      "        [130., 130., 129., 129.]], device='cuda:0', grad_fn=<UnbindBackward0>), shape: torch.Size([8100, 4]), requires_grad: True\n",
      "\u001b[96m -------------------- \u001b[0m\n",
      "\u001b[96m Line:  print2((f\"hist_p_nondiff: {hist_p_nondiff}, shape: {hist_p_nondiff.shape}, sum: {torch.sum(hist_p_nondiff)}, requires_grad: {hist_p_nondiff.requires_grad}\", ), self._should_log > 1) \u001b[0m\n",
      "hist_p_nondiff: tensor([    0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,  1353., 11882., 13595.,  5570.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.], device='cuda:0'), shape: torch.Size([256]), sum: 32400.0, requires_grad: False\n",
      "\u001b[96m -------------------- \u001b[0m\n",
      "tensor([[132, 132, 132, 132],\n",
      "        [132, 132, 132, 132],\n",
      "        [132, 132, 132, 132],\n",
      "        ...,\n",
      "        [131, 131, 130, 130],\n",
      "        [131, 131, 130, 130],\n",
      "        [131, 131, 130, 130]], device='cuda:0') torch.Size([8100, 4])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([8100, 4]) torch.Size([8100, 4])\n",
      "torch.Size([8100, 4])\n",
      "torch.Size([8100, 8100, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (4) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     should_calc_hists \u001b[39m=\u001b[39m ((e \u001b[39m==\u001b[39m hyperparameters[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mor\u001b[39;00m (e \u001b[39m%\u001b[39m histogram_rate \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m early_stopper\u001b[39m.\u001b[39mearly_stop)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     _, _, train_collisions, train_min_possible_collisions, train_hists, train_collisions_losses, train_kl_losses, train_sigma_losses, train_loss \u001b[39m=\u001b[39m train_loop(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmultires,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39m# Calc histograns at: first epoch, last epoch, every histogram_rate epochs, every time early stopper stops\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         should_calc_hists\u001b[39m=\u001b[39;49mshould_calc_hists,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         should_kl_hist\u001b[39m=\u001b[39;49mhyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mkl_hist_loss\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         should_log\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39mif\u001b[39;49;00m should_log \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     _, _, test_collisions, test_min_possible_collisions, test_hists, test_collisions_losses, test_kl_losses, test_sigma_losses, test_loss \u001b[39m=\u001b[39m test_loop(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         x\u001b[39m=\u001b[39meval_x,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         model\u001b[39m=\u001b[39mmultires,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39m# should_log=1 if should_log else 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mif\u001b[39;00m should_wandb:\n",
      "\u001b[1;32m/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb Cell 47\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m out, unique_hashed, unique_probs, unique_sigmas, collisions, min_possible_collisions, hists \u001b[39m=\u001b[39m multires(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     x, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     should_calc_hists\u001b[39m=\u001b[39mshould_calc_hists \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# print(\"out shape:\", out.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# print(f\"Unique hashed: {[unique_hashed[l].shape for l in range(len(unique_hashed))]}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# print(f\"Unique probs: {unique_probs[0], unique_probs[0].shape}\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m collisions_losses, kl_losses, sigma_loss, loss \u001b[39m=\u001b[39m loss_fn(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     collisions, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     min_possible_collisions, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     unique_probs \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m should_kl_hist \u001b[39melse\u001b[39;49;00m rearrange(out, \u001b[39m\"\u001b[39;49m\u001b[39mbatch pixels levels verts xyz -> batch levels pixels (verts xyz)\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m], \u001b[39m#unique_hashed,  \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     unique_sigmas\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/collision_handling_in_instantNGP/project_course/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/collision_handling_in_instantNGP/project_course/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb Cell 47\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m print2((\u001b[39m\"\u001b[39m\u001b[39mCollisions Losses:\u001b[39m\u001b[39m\"\u001b[39m, collisions_losses), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_log \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m print2((\u001b[39m\"\u001b[39m\u001b[39mCollisions Losses grads:\u001b[39m\u001b[39m\"\u001b[39m, collisions_losses\u001b[39m.\u001b[39mrequires_grad), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_log \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m kl_div_losses \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m     (\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kl_div(prob\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], prob\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_kl_hist \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kl_div(\u001b[39m0\u001b[39m, prob)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mfor\u001b[39;00m prob \u001b[39min\u001b[39;00m probs\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m print2((\u001b[39m\"\u001b[39m\u001b[39mKL Losses:\u001b[39m\u001b[39m\"\u001b[39m, kl_div_losses), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_log \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, bcolors\u001b[39m.\u001b[39mFAIL)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m sigma_losses \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmse_loss(sigma, torch\u001b[39m.\u001b[39mzeros_like(sigma))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39mfor\u001b[39;00m sigma \u001b[39min\u001b[39;00m sigmas\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m ])\n",
      "\u001b[1;32m/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb Cell 47\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m print2((\u001b[39m\"\u001b[39m\u001b[39mCollisions Losses:\u001b[39m\u001b[39m\"\u001b[39m, collisions_losses), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_log \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m print2((\u001b[39m\"\u001b[39m\u001b[39mCollisions Losses grads:\u001b[39m\u001b[39m\"\u001b[39m, collisions_losses\u001b[39m.\u001b[39mrequires_grad), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_log \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m kl_div_losses \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m     (\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kl_div(prob\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], prob\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_kl_hist \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_kl_div(\u001b[39m0\u001b[39;49m, prob)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mfor\u001b[39;00m prob \u001b[39min\u001b[39;00m probs\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m print2((\u001b[39m\"\u001b[39m\u001b[39mKL Losses:\u001b[39m\u001b[39m\"\u001b[39m, kl_div_losses), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_log \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, bcolors\u001b[39m.\u001b[39mFAIL)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m sigma_losses \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmse_loss(sigma, torch\u001b[39m.\u001b[39mzeros_like(sigma))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39mfor\u001b[39;00m sigma \u001b[39min\u001b[39;00m sigmas\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m ])\n",
      "\u001b[1;32m/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m print2((\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhist_p_nondiff: \u001b[39m\u001b[39m{\u001b[39;00mhist_p_nondiff\u001b[39m}\u001b[39;00m\u001b[39m, shape: \u001b[39m\u001b[39m{\u001b[39;00mhist_p_nondiff\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, sum: \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39msum(hist_p_nondiff)\u001b[39m}\u001b[39;00m\u001b[39m, requires_grad: \u001b[39m\u001b[39m{\u001b[39;00mhist_p_nondiff\u001b[39m.\u001b[39mrequires_grad\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, ), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_log \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m# Other differentiable histogram methods\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39m# hist_p = self.hist_c_differentiable(p, bins=self._hash_table_size)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39m# hist_p = self.differentiable_hist(p)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=133'>134</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39m# hist_p = differentiable_histogram(p, bins=self._hash_table_size, min=0, max=self._hash_table_size).squeeze(0).squeeze(0)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=135'>136</a>\u001b[0m hist_p \u001b[39m=\u001b[39m differentiable_histogram_optimized(p, bins\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_hash_table_size, \u001b[39mmin\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39mmax\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_hash_table_size)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m print2((\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhist_p: \u001b[39m\u001b[39m{\u001b[39;00mhist_p\u001b[39m}\u001b[39;00m\u001b[39m, shape: \u001b[39m\u001b[39m{\u001b[39;00mhist_p\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, sum: \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39msum(hist_p)\u001b[39m}\u001b[39;00m\u001b[39m, requires_grad: \u001b[39m\u001b[39m{\u001b[39;00mhist_p\u001b[39m.\u001b[39mrequires_grad\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, ), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_log \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCount(hist_p_nondiff != hist_p):\u001b[39m\u001b[39m\"\u001b[39m, hist_p_nondiff\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39meq(hist_p_nondiff, hist_p)))\n",
      "\u001b[1;32m/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39mprint\u001b[39m((torch\u001b[39m.\u001b[39mcumsum(x, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:, bin_indices] \u001b[39m*\u001b[39m mask_sub)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39m# hist_torch += torch.cumsum(x, dim=-1)[:, :, bin_indices] * mask_sub\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39m# hist_torch[:, :, 1:] -= torch.cumsum(x, dim=-1)[:, :, bin_indices - 1] * mask_sub\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m \u001b[39m# hist_torch[:, :, :-1] += torch.cumsum(x, dim=-1)[:, :, bin_indices + 1] * mask_plus\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m \u001b[39m# hist_torch[:, :, -1] -= torch.cumsum(x, dim=-1)[:, :, bin_indices] * mask_plus\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m hist_torch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcumsum(x, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:, bin_indices] \u001b[39m*\u001b[39m mask_sub\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m hist_torch[:, :, \u001b[39m1\u001b[39m:] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcumsum(x, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:, bin_indices \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m mask_sub\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.196.36.18/home/federicomontagna/collision_handling_in_instantNGP/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m hist_torch[:, :, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcumsum(x, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:, bin_indices \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m mask_plus\n",
      "File \u001b[0;32m~/collision_handling_in_instantNGP/project_course/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (4) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "plt.ioff()\n",
    "should_calc_hists = False\n",
    "\n",
    "pbar = tqdm(range(0, hyperparameters[\"epochs\"]))\n",
    "\n",
    "for e in pbar:\n",
    "\n",
    "    should_calc_hists = ((e == hyperparameters[\"epochs\"] - 1) or (e % histogram_rate == 0) or early_stopper.early_stop)\n",
    "    \n",
    "    _, _, train_collisions, train_min_possible_collisions, train_hists, train_collisions_losses, train_kl_losses, train_sigma_losses, train_loss = train_loop(\n",
    "        x=x,\n",
    "        model=multires,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        # Calc histograns at: first epoch, last epoch, every histogram_rate epochs, every time early stopper stops\n",
    "        should_calc_hists=should_calc_hists,\n",
    "        should_kl_hist=hyperparameters[\"kl_hist_loss\"],\n",
    "        should_log=1 if should_log else 0\n",
    "    )\n",
    "\n",
    "    _, _, test_collisions, test_min_possible_collisions, test_hists, test_collisions_losses, test_kl_losses, test_sigma_losses, test_loss = test_loop(\n",
    "        x=eval_x,\n",
    "        model=multires,\n",
    "        loss_fn=loss_fn,\n",
    "        # Calc histograns at: first epoch, last epoch, every histogram_rate epochs, every time early stopper stops\n",
    "        should_calc_hists=should_calc_hists,\n",
    "        should_kl_hist=hyperparameters[\"kl_hist_loss\"],\n",
    "        should_log=1 if should_log else 0\n",
    "    )\n",
    "\n",
    "    if should_wandb:\n",
    "        wandb_log(\n",
    "            e=e,\n",
    "            train_loss=train_loss,\n",
    "            train_collisions=train_collisions,\n",
    "            train_min_possible_collisions=train_min_possible_collisions,\n",
    "            train_collisions_losses=train_collisions_losses,\n",
    "            train_kl_losses=train_kl_losses,\n",
    "            train_sigma_losses=train_sigma_losses,\n",
    "            train_hists=train_hists,\n",
    "            test_loss=test_loss,\n",
    "            test_collisions=test_collisions,\n",
    "            test_min_possible_collisions=test_min_possible_collisions,\n",
    "            test_collisions_losses=test_collisions_losses,\n",
    "            test_kl_losses=test_kl_losses,\n",
    "            test_sigma_losses=test_sigma_losses,\n",
    "            test_hists=test_hists,\n",
    "            should_log_hists=should_calc_hists\n",
    "        )\n",
    "\n",
    "    pbar.set_description(f\"Epoch {e}, Loss: {test_loss}, Collisions: {test_collisions}\")\n",
    "\n",
    "    # for name, param in multires.named_parameters():\n",
    "    #     print(f'Parameter: {name}, Gradient: {param.grad}')\n",
    "\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"!!! Stopping at epoch:\", e, \"!!!\")\n",
    "        break\n",
    "\n",
    "    early_stopper(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_wandb:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_course",
   "language": "python",
   "name": "project_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
