{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeRF: Collision handling in Instant Neural Graphics Primitives\n",
    "#### Federico Montagna (fedemonti00@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from math import atan2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MultipleLocator, AutoMinorLocator, FixedLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections, functools, operator\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import io, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.profiler as profiler\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "import traceback\n",
    "from pprint import pprint\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pdb\n",
    "\n",
    "try:\n",
    "    from zoneinfo import ZoneInfo\n",
    "except ImportError:\n",
    "    from backports.zoneinfo import ZoneInfo\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# from params import *\n",
    "\n",
    "print(\"Cuda avilable:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Available device {i}:\", torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device {torch.cuda.current_device()}:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "torch.set_default_device(device)\n",
    "\n",
    "random_seed = np.random.randint(0, (2**16 - 1)) #2**16 - 1\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.random.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "print(\"Random seed:\", random_seed)\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"main.ipynb\"\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "def print2(texts, log: bool = False, color: bcolors = bcolors.OKCYAN) -> None:\n",
    "    if log:\n",
    "        stack = traceback.extract_stack()\n",
    "        calling_frame = stack[-2]\n",
    "        calling_line = calling_frame.line\n",
    "        print(color, \"Line: \", calling_line, bcolors.ENDC)\n",
    "        for text in texts:\n",
    "            print(text)\n",
    "        print(color, \"-\"*20, bcolors.ENDC)\n",
    "\n",
    "\n",
    "def print_allocated_memory(log: bool = True):\n",
    "    if log:\n",
    "        stack = traceback.extract_stack()\n",
    "        calling_frame = stack[-2]\n",
    "        calling_line = calling_frame.line\n",
    "        print(bcolors.HEADER, \"Line: \", calling_line, bcolors.ENDC)\n",
    "\n",
    "        allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to gigabytes\n",
    "        print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert to gigabytes\n",
    "        print(f\"Peak Allocated Memory: {peak_memory:.2f} GB\")\n",
    "\n",
    "        print(bcolors.OKCYAN, \"-\"*20, bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load wandb apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apikey_path = \".wandb_apikey.txt\"\n",
    "# if os.path.exists(apikey_path):\n",
    "#     with open(apikey_path, \"r\") as f:\n",
    "#         apikey = f.read()\n",
    "#         !wandb login {apikey} # --relogin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        dir_name: str,\n",
    "        images_names: List[str],\n",
    "        should_random_permute_input: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        root : str\n",
    "            Path to root directory.\n",
    "        dir_name : str\n",
    "            Name of directory.\n",
    "        images_names : List[str]\n",
    "            List of images names.\n",
    "        should_random_permute_input : bool, optional (default is False)\n",
    "            Should random permute input\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        self._root: str = root\n",
    "        self._dir_name: str = dir_name\n",
    "        self._images_names: List[str] = images_names\n",
    "\n",
    "        self._should_random_permute_input: bool = should_random_permute_input\n",
    "\n",
    "        self._images_paths: List[str] = [\n",
    "            os.path.join(self._root, self._dir_name, image_name)\n",
    "            for image_name in self._images_names\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self, idx: torch.Tensor or int) -> Tuple[torch.Tensor, torch.Tensor, int, int]:\n",
    "        \"\"\"\n",
    "        Get data by indices.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : torch.Tensor or int\n",
    "            Indices of data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor, int, int]\n",
    "            Tuple of images.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If images have different sizes.\n",
    "        \"\"\"\n",
    "\n",
    "        if idx == -1:\n",
    "            idx = torch.arange(len(self._images_paths))\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx\n",
    "        else:\n",
    "            idx = torch.tensor(idx)\n",
    "\n",
    "        images: List = [\n",
    "            rearrange(io.read_image(self._images_paths[_id]).to(device), \"rgb h w -> h w rgb\")\n",
    "            for _id in idx\n",
    "        ]\n",
    "\n",
    "        heights, widths = list(set([image.shape[0] for image in images])), list(set([image.shape[1] for image in images]))\n",
    "\n",
    "        if len(heights) > 1 or len(widths) > 1:\n",
    "            raise ValueError(\"Images have different sizes.\")\n",
    "        else:\n",
    "            h = heights[0]\n",
    "            w = widths[0]\n",
    "\n",
    "        imgs_shape = h * w\n",
    "        \n",
    "        reordered_indices: torch.Tensor = torch.stack([\n",
    "            torch.zeros((imgs_shape, )).int()\n",
    "            for _ in images\n",
    "        ])\n",
    "\n",
    "        X: torch.Tensor = torch.zeros(idx.shape[0], imgs_shape, 2)\n",
    "        Y: torch.Tensor = torch.zeros(len(images), imgs_shape, images[0].shape[-1])\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            if self._should_random_permute_input:\n",
    "                shuffled_indices = torch.randperm(imgs_shape).int()\n",
    "            else:\n",
    "                shuffled_indices = torch.arange(imgs_shape).int()\n",
    "            \n",
    "            reordered_indices[i][shuffled_indices] = torch.arange(imgs_shape).int()\n",
    "            \n",
    "            X[i] = torch.tensor(\n",
    "                np.stack(np.meshgrid(range(h), range(w), indexing=\"ij\"), axis=-1).reshape(-1, 2)\n",
    "            )[shuffled_indices]\n",
    "\n",
    "            Y[i] = rearrange(image, \"h w rgb -> (h w) rgb\")[shuffled_indices]\n",
    "\n",
    "        X = (\n",
    "            X.float() / (max(w, h)) # No (max(w, h) - 1)\n",
    "        ).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        Y = Y.float() / 255\n",
    "        \n",
    "        # else:\n",
    "        #     X: torch.Tensor = (\n",
    "        #         torch.stack([\n",
    "        #             torch.tensor(\n",
    "        #                 np.stack(np.meshgrid(range(h), range(w), indexing=\"ij\"), axis=-1).reshape(-1, 2)\n",
    "        #             )\n",
    "        #             for _ in images\n",
    "        #         ]).float() / (max(w, h)) # No (max(w, h) - 1)\n",
    "        #     ).unsqueeze(-1).unsqueeze(-1) #.requires_grad_()\n",
    "            \n",
    "        #     Y: torch.Tensor = torch.stack(\n",
    "        #         images\n",
    "        #     ).float() / 255\n",
    "\n",
    "        return X, Y, h, w, reordered_indices, self._images_names\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self._images_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass Differentiable Approximation\n",
    "[https://github.com/kitayama1234/Pytorch-BPDA]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPDA(x, round_function):\n",
    "    forward_value = round_function(x)\n",
    "    out = x.clone()\n",
    "    out.data = forward_value.data\n",
    "    return out\n",
    "\n",
    "def differentiable_floor(x, round_function=torch.floor):\n",
    "    return BPDA(x, round_function)\n",
    "\n",
    "def differentiable_round(x, round_function=torch.round):\n",
    "    return BPDA(x, round_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Convolution Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianConvolution(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor, hash_table_size: int):\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        ctx.save_for_backward(x)\n",
    "        ctx.hash_table_size = hash_table_size\n",
    "\n",
    "        indices = differentiable_round(x * hash_table_size)\n",
    "\n",
    "        # ctx.indices = indices\n",
    "        # ctx.save_for_backward(indices)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        # indices = ctx.saved_tensors\n",
    "        x, = ctx.saved_tensors\n",
    "        hash_table_size = ctx.hash_table_size\n",
    "\n",
    "        indices = x * hash_table_size\n",
    "        # indices = ctx.indices\n",
    "\n",
    "        grad_x = grad_output * norm.pdf(np.arange(0, hash_table_size, 1), loc=indices, scale=1)\n",
    "\n",
    "        return grad_x, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Differentiable Histogram Counting Method\n",
    "#############################################\n",
    "# https://github.com/hyk1996/pytorch-differentiable-histogram\n",
    "def differentiable_histogram(x, bins=255, min=0.0, max=1.0):\n",
    "\n",
    "    if len(x.shape) == 4:\n",
    "        n_samples, n_chns, _, _ = x.shape\n",
    "    elif len(x.shape) == 2:\n",
    "        n_samples, n_chns = 1, 1\n",
    "    else:\n",
    "        raise AssertionError('The dimension of input tensor should be 2 or 4.')\n",
    "\n",
    "    hist_torch = torch.zeros(n_samples, n_chns, bins).to(x.device)\n",
    "    delta = (max - min) / bins\n",
    "\n",
    "    BIN_Table = torch.arange(start=0, end=bins, step=1) * delta\n",
    "\n",
    "    for dim in range(1, bins-1, 1):\n",
    "        h_r = BIN_Table[dim].item()             # h_r\n",
    "        h_r_sub_1 = BIN_Table[dim - 1].item()   # h_(r-1)\n",
    "        h_r_plus_1 = BIN_Table[dim + 1].item()  # h_(r+1)\n",
    "\n",
    "        mask_sub = ((h_r > x) & (x >= h_r_sub_1)).float()\n",
    "        mask_plus = ((h_r_plus_1 > x) & (x >= h_r)).float()\n",
    "\n",
    "        hist_torch[:, :, dim] += torch.sum(((x - h_r_sub_1) * mask_sub).view(n_samples, n_chns, -1), dim=-1)\n",
    "        hist_torch[:, :, dim] += torch.sum(((h_r_plus_1 - x) * mask_plus).view(n_samples, n_chns, -1), dim=-1)\n",
    "\n",
    "    return hist_torch / delta\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Kornia Differentiable Histogram Counting Method\n",
    "#############################################\n",
    "# https://kornia.readthedocs.io/en/latest/_modules/kornia/enhance/histogram.html#histogram\n",
    "\n",
    "def marginal_pdf(\n",
    "    values: torch.Tensor, bins: torch.Tensor, sigma: torch.Tensor, epsilon: float = 1e-10\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Calculate the marginal probability distribution function of the input tensor based on the number of\n",
    "    histogram bins.\n",
    "\n",
    "    Args:\n",
    "        values: shape [BxNx1].\n",
    "        bins: shape [NUM_BINS].\n",
    "        sigma: shape [1], gaussian smoothing factor.\n",
    "        epsilon: scalar, for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]:\n",
    "          - torch.Tensor: shape [BxN].\n",
    "          - torch.Tensor: shape [BxNxNUM_BINS].\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(values, torch.Tensor):\n",
    "        raise TypeError(f\"Input values type is not a torch.Tensor. Got {type(values)}\")\n",
    "\n",
    "    if not isinstance(bins, torch.Tensor):\n",
    "        raise TypeError(f\"Input bins type is not a torch.Tensor. Got {type(bins)}\")\n",
    "\n",
    "    if not isinstance(sigma, torch.Tensor):\n",
    "        raise TypeError(f\"Input sigma type is not a torch.Tensor. Got {type(sigma)}\")\n",
    "\n",
    "    if not values.dim() == 3:\n",
    "        raise ValueError(f\"Input values must be a of the shape BxNx1. Got {values.shape}\")\n",
    "\n",
    "    if not bins.dim() == 1:\n",
    "        raise ValueError(f\"Input bins must be a of the shape NUM_BINS. Got {bins.shape}\")\n",
    "\n",
    "    if not sigma.dim() == 0:\n",
    "        raise ValueError(f\"Input sigma must be a of the shape 1. Got {sigma.shape}\")\n",
    "\n",
    "    residuals = values - bins.unsqueeze(0).unsqueeze(0)\n",
    "    kernel_values = torch.exp(-0.5 * (residuals / sigma).pow(2))\n",
    "\n",
    "    pdf = torch.mean(kernel_values, dim=1)\n",
    "    normalization = torch.sum(pdf, dim=1).unsqueeze(1) + epsilon\n",
    "    pdf = pdf / normalization\n",
    "\n",
    "    return pdf, kernel_values\n",
    "\n",
    "def histogram(x: torch.Tensor, bins: torch.Tensor, bandwidth: torch.Tensor = None, epsilon: float = 1e-10) -> torch.Tensor:\n",
    "    \"\"\"Estimate the NORMALIZED histogram of the input tensor.\n",
    "\n",
    "    The calculation uses kernel density estimation which requires a bandwidth (smoothing) parameter.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor to compute the histogram with shape :math:`(B, D)`.\n",
    "        bins: The number of bins to use the histogram :math:`(N_{bins})`.\n",
    "        bandwidth: Gaussian smoothing factor with shape shape [1].\n",
    "        epsilon: A scalar, for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        Computed histogram of shape :math:`(B, N_{bins})`.\n",
    "\n",
    "    Examples:\n",
    "        >>> x = torch.rand(1, 10)\n",
    "        >>> bins = torch.torch.linspace(0, 255, 128)\n",
    "        >>> hist = histogram(x, bins, bandwidth=torch.tensor(0.9))\n",
    "        >>> hist.shape\n",
    "        torch.Size([1, 128])\n",
    "    \"\"\"\n",
    "\n",
    "    # x = rearrange(x, \"b d -> 1 (b d)\")\n",
    "    # x = rearrange(x, \"b d -> d b\")\n",
    "    # x = rearrange(x, \"b d -> (d b) 1\")\n",
    "    # print(x.shape, bins.shape, bandwidth.shape)\n",
    "\n",
    "    N = x.size(1)\n",
    "    if bandwidth is None:\n",
    "        std = torch.std(x)\n",
    "        q25 = np.percentile(x.detach().cpu().numpy(), 25)\n",
    "        q75 = np.percentile(x.detach().cpu().numpy(), 75)\n",
    "        iqr = torch.tensor(q75 - q25)\n",
    "        bandwidth = 0.9 * torch.min(std, iqr / 1.34) * (N **(-0.2))\n",
    "\n",
    "    # print(sigma_initial)\n",
    "\n",
    "    pdf, _ = marginal_pdf(x.unsqueeze(2), bins, bandwidth, epsilon)\n",
    "    # print(\"PDF:\", pdf, pdf.shape)\n",
    "\n",
    "    return pdf * (N + epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Student's T Table)[https://www.craftonhills.edu/current-students/tutoring-center/mathematics-tutoring/distribution_tables_normal_studentt_chisquared.pdf]  \n",
    "$\\alpha = 0.8$, $DF = +\\inf$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashFunction(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layers_widths: List[int],\n",
    "        input_dim: int = 2,\n",
    "        output_dim: int = 1,\n",
    "        hash_table_size: int = 2**14,\n",
    "        sigma_scale: float = 1.0,\n",
    "        should_log: int = 0, \n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Hash function module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_layers_widths : List[int]\n",
    "            List of hidden layers widths.\n",
    "        input_dim : int, optional (default is 2)\n",
    "            Input dimension\n",
    "        output_dim : int, optional (default is 1)\n",
    "            Output dimension\n",
    "        hash_table_size : int, optional (default is 2**14)\n",
    "            Hash table size.\n",
    "        sigma_scale : float, optional (default is 1.0)\n",
    "            Sigma scale.\n",
    "        should_log : int, optional (default is 0)\n",
    "            - 0: No logging.\n",
    "            - > 0: Log forward pass.\n",
    "            - > 1: Log layers grads and outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        super(HashFunction, self).__init__()\n",
    "\n",
    "        self._hash_table_size: int = hash_table_size - 1\n",
    "        self._sigma_scale: float = sigma_scale\n",
    "\n",
    "        self._alpha: float = 0.8 # student's t-distribution confidence level\n",
    "        \n",
    "        self._should_log: int = should_log\n",
    "\n",
    "        layers_widths: List[int] = [input_dim, *hidden_layers_widths, output_dim]\n",
    "\n",
    "        self.module_list: nn.ModuleList = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=layers_widths[i], out_features=layers_widths[i + 1]),\n",
    "                nn.ReLU() if (i < (len(layers_widths) - 2)) else nn.Sigmoid()\n",
    "            ) for i in range(len(layers_widths) - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        print2((\"Input:\", x, x.shape), (self._should_log > 0))\n",
    "        print2((\"Input grad:\", x.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        for i, layer in enumerate(self.module_list):\n",
    "            x = layer(x)\n",
    "\n",
    "        print2((f\"After layers:\", f\"Output: {x}, shape: {x.shape}\"), (self._should_log > 0))\n",
    "        print2((f\"After layers:\", f\"Grad info: {x.requires_grad}, {x.grad_fn}\"), (self._should_log > 1))\n",
    "        \n",
    "        # x = torch.nan_to_num(x) # Sanitize nan to 0.0\n",
    "\n",
    "        if x.shape[-1] == 1: # directly indices\n",
    "            indices = GaussianConvolution.apply(x, self._hash_table_size)\n",
    "            print2((\"GaussianConvolution:\", indices, indices.shape), (self._should_log > 0))\n",
    "            print2((\"GaussianConvolution grad:\", indices.requires_grad, ), (self._should_log > 1))\n",
    "\n",
    "            sigma = None\n",
    "        else: # mu and sigma            \n",
    "            x = x.unsqueeze(-1)\n",
    "            print2((\"x:\", x, x.shape), (self._should_log > 0))\n",
    "\n",
    "            sigma = (x[..., 1, :] * self._sigma_scale)\n",
    "            x = x[..., 0, :]\n",
    "            mu = (differentiable_round(x * self._hash_table_size))\n",
    "\n",
    "            print2((\"Mu:\", mu, mu.shape), (self._should_log > 0))\n",
    "            print2((\"Sigma:\", sigma, sigma.shape), (self._should_log > 0))\n",
    "\n",
    "            a, b = norm.interval(self._alpha, loc=mu.detach().cpu().numpy(), scale=sigma.detach().cpu().numpy()) # a -> at left of mean, b -> at right of mean\n",
    "            a, b = np.round(a), np.round(b)\n",
    "\n",
    "            print2((\"a:\", a, a.shape), (self._should_log > 0))\n",
    "            print2((\"b:\", b, b.shape), (self._should_log > 0))\n",
    "\n",
    "            indices = mu\n",
    "\n",
    "            del mu, a, b\n",
    "\n",
    "        return x, indices, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiresolution(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_min: int,\n",
    "        n_max: int,\n",
    "        num_levels: int,\n",
    "        HashFunction: HashFunction,\n",
    "        hash_table_size: int = 2**14,\n",
    "        input_dim: int = 2,\n",
    "        should_use_all_levels: bool = False,\n",
    "        should_normalize_levels: bool = False,\n",
    "        should_fast_hash: bool = False,\n",
    "        should_log: int = 0\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Multiresolution module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_min : int\n",
    "            Minimum scaling factor.\n",
    "        n_max : int\n",
    "            Maximum scaling factor.\n",
    "        num_levels : int\n",
    "            Number of levels.\n",
    "        HashFunction : HashFunction\n",
    "            Hash function module.\n",
    "        hash_table_size : int, optional (default is 2**14)\n",
    "            Hash table size.\n",
    "        input_dim : int, optional (default is 2)\n",
    "            Input dimension.\n",
    "        should_use_all_levels : bool, optional (default is False)\n",
    "            Whether to use all levels or only the ones with collisions.\n",
    "        should_normalize_levels : bool, optional (default is False)\n",
    "            Whether to normalize levels or not.\n",
    "        should_fast_hash : bool, optional (default is False)\n",
    "            Whether to use fast hash instead of HashFunction or not.\n",
    "        should_log : int, optional (default is 0)\n",
    "            - 0: No logging.\n",
    "            - > 0: Log forward pass.\n",
    "            - > 1: Log helper functions.\n",
    "            - > 2: Log collisions.\n",
    "            - > 3: Log dummies.\n",
    "            - > 5: Log initialization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        super(Multiresolution, self).__init__()\n",
    "\n",
    "        self.HashFunction: nn.Module = HashFunction\n",
    "\n",
    "        self._hash_table_size: int = hash_table_size\n",
    "        self._num_levels: int = num_levels\n",
    "        self._input_dim: int = input_dim\n",
    "        self._should_use_all_levels: bool = should_use_all_levels\n",
    "        self._should_normalize_levels: bool = should_normalize_levels\n",
    "        self._should_fast_hash: bool = should_fast_hash\n",
    "        self._should_log: int = should_log\n",
    "\n",
    "        b: torch.Tensor = torch.tensor(np.exp((np.log(n_max) - np.log(n_min)) / (self._num_levels - 1))).float()\n",
    "        if b > 2 or b <= 1:\n",
    "            print(\n",
    "                f\"The between level scale is recommended to be <= 2 and needs to be > 1 but was {b:.4f}.\"\n",
    "            )\n",
    "        \n",
    "        self._levels: torch.Tensor = torch.stack([\n",
    "            torch.floor(n_min * (b ** l)) for l in range(self._num_levels)\n",
    "        ]).reshape(1, 1, -1, 1)\n",
    "        print2((\"Levels:\", self._levels, self._levels.shape), (self._should_log > 5))\n",
    "        print2((\"Levels grads:\", self._levels.requires_grad, ), (self._should_log > 5))\n",
    "\n",
    "        self._voxels_helper_hypercube: torch.Tensor = rearrange(\n",
    "            torch.tensor(\n",
    "                np.stack(np.meshgrid(range(2), range(2), range(self._input_dim - 1), indexing=\"ij\"), axis=-1)\n",
    "            ),\n",
    "            \"cols rows depths verts -> (depths rows cols) verts\"\n",
    "        ).T[:self._input_dim, :].unsqueeze(0).unsqueeze(2)\n",
    "        print2((\"voxels_helper_hypercube:\", self._voxels_helper_hypercube, self._voxels_helper_hypercube.shape), (self._should_log > 5))\n",
    "        print2((\"voxels_helper_hypercube grads:\", self._voxels_helper_hypercube.requires_grad), (self._should_log > 5))\n",
    "\n",
    "        if self._should_fast_hash:\n",
    "            self._prime_numbers = torch.nn.Parameter(\n",
    "                torch.from_numpy(\n",
    "                    np.array([1, 2654435761, 805459861])\n",
    "                ).to(device),\n",
    "                False\n",
    "            )\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        should_calc_hists: bool = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        # print2((\"x:\", x, x.shape), (self._should_log > 0))\n",
    "        print2((\"x grads:\", x.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        _, grid_coords = self._scale_to_grid(x)\n",
    "        dummy_grids, og_indices, _ = self._calc_dummies(grid_coords=grid_coords)\n",
    "        min_possible_collisions, _ = self.calc_hash_collisions(dummy_grids=dummy_grids)\n",
    "\n",
    "        if self._should_fast_hash:\n",
    "            hashed = self._fast_hash(grid_coords)\n",
    "            probs = torch.ones_like(hashed)\n",
    "            sigmas = torch.zeros_like(hashed)\n",
    "        else:\n",
    "            probs, hashed, sigmas = (\n",
    "                self.HashFunction(grid_coords) \n",
    "                if self._should_use_all_levels \n",
    "                else \n",
    "                self._multiresolution_hash(grid_coords, min_possible_collisions)\n",
    "            )\n",
    "\n",
    "        print2((\"hashed:\", hashed, hashed.shape), (self._should_log > 0))\n",
    "        print2((\"hashed grads:\", hashed.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        print2((\"probs:\", probs, probs.shape), (self._should_log > 0))\n",
    "        print2((\"probs grads:\", probs.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        print2((\"sigmas:\", sigmas, sigmas.shape), (self._should_log > 0))\n",
    "        print2((\"sigmas grads:\", sigmas.requires_grad, ), (self._should_log > 0))\n",
    "\n",
    "        _, _, dummy_hashed = self._calc_dummies(hashed=hashed)\n",
    "\n",
    "        _, collisions = self.calc_hash_collisions(dummy_grids=dummy_grids, dummy_hashed=dummy_hashed)\n",
    "        del dummy_hashed\n",
    "\n",
    "        unique_probs, unique_hashed, unique_sigmas = self._calc_uniques(probs, hashed, sigmas, og_indices)\n",
    "        del og_indices\n",
    "\n",
    "        if should_calc_hists:\n",
    "            hists = self._hist_collisions(dummy_grids, unique_hashed, min_possible_collisions, should_show=False)\n",
    "        else:\n",
    "            hists = None\n",
    "        del dummy_grids, sigmas\n",
    "\n",
    "        return hashed, unique_hashed, unique_probs, unique_sigmas, collisions, min_possible_collisions, hists\n",
    "\n",
    "    def _multiresolution_hash(self, x: torch.Tensor, min_possible_collision: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Multiresolution hash function.\n",
    "        Calculates the hash of the input tensor only for levels with more than 0 min_possible_collisions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor.\n",
    "        min_possible_collision : torch.Tensor\n",
    "            Minimum possible collisions.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Hashed tensor, probabilities and sigmas.\n",
    "        \"\"\"\n",
    "\n",
    "        hashed = torch.zeros((x.shape[0], x.shape[1], self._num_levels, self._input_dim**2, 1))\n",
    "        probs = torch.ones((x.shape[0], x.shape[1], self._num_levels, self._input_dim**2, 1))\n",
    "        sigmas = torch.zeros((x.shape[0], x.shape[1], self._num_levels, self._input_dim**2, 1))\n",
    "\n",
    "        x_non_collisions_levels = x[:, :, min_possible_collision <= 0, :, :]\n",
    "\n",
    "        if self._should_normalize_levels:\n",
    "            x_non_collisions_levels = x_non_collisions_levels * self._levels.unsqueeze(-1)[:, :, min_possible_collision <= 0, :, :]\n",
    "\n",
    "        # TODO: check, input is ij so maybe (i + j * level)?\n",
    "        hashed_non_collisions_levels = (\n",
    "            (\n",
    "                (self._levels[:, :, min_possible_collision <= 0, :] + 1) \n",
    "                * \n",
    "                x_non_collisions_levels[:, :, :, :, 0]\n",
    "            ) \n",
    "            + \n",
    "            x_non_collisions_levels[:, :, :, :, 1]\n",
    "        )\n",
    "\n",
    "        print2((\"Hashed indices of levels without collisions:\", hashed_non_collisions_levels, hashed_non_collisions_levels.shape), (self._should_log > 5))\n",
    "        hashed[:, :, min_possible_collision <= 0, :, :] = hashed_non_collisions_levels.unsqueeze(-1)\n",
    "\n",
    "        probs_collsions_levels, hashed_collisions_levels, sigmas_collisions_levels = self.HashFunction(x[:, :, min_possible_collision > 0, :, :])\n",
    "        probs[:, :, min_possible_collision > 0, :, :] = probs_collsions_levels\n",
    "        sigmas[:, :, min_possible_collision > 0, :, :] = sigmas_collisions_levels\n",
    "        hashed[:, :, min_possible_collision > 0, :, :] = hashed_collisions_levels\n",
    "\n",
    "        del hashed_non_collisions_levels, probs_collsions_levels, hashed_collisions_levels, sigmas_collisions_levels\n",
    "\n",
    "        return probs, hashed, sigmas\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _scale_to_grid(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Scale coordinates to grid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Original coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]\n",
    "            Scaled coordinates, grid coordinates and dummy grids.\n",
    "        \"\"\"\n",
    "\n",
    "        scaled_coords: torch.Tensor = x.float() * self._levels.float()\n",
    "        print2((\"scaled_coords:\", scaled_coords, scaled_coords.shape), (self._should_log > 1))\n",
    "        print2((\"scaled_coords grads:\", scaled_coords.requires_grad, ), (self._should_log > 1))\n",
    "\n",
    "        grid_coords: torch.Tensor = rearrange(\n",
    "            torch.add(\n",
    "                torch.floor(scaled_coords),\n",
    "                self._voxels_helper_hypercube\n",
    "            ),\n",
    "            \"batch pixels xyz levels verts -> batch pixels levels verts xyz\"\n",
    "        )\n",
    "        \n",
    "        if self._should_normalize_levels:\n",
    "            grid_coords = grid_coords / self._levels.unsqueeze(-1)\n",
    "\n",
    "        print2((\"grid_coords:\", grid_coords, grid_coords.shape), (self._should_log > 1))\n",
    "        print2((\"grid_coords grads:\", grid_coords.requires_grad, ), (self._should_log > 1))\n",
    "\n",
    "        return _, grid_coords\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _calc_dummies(\n",
    "        self,\n",
    "        grid_coords: torch.Tensor | None = None,\n",
    "        hashed: torch.Tensor | None = None,\n",
    "    ) -> Tuple[List[torch.Tensor] | None, List[torch.Tensor] | None, List[torch.Tensor] | None]:\n",
    "        \"\"\"\n",
    "        Calculate dummies.\n",
    "        If grid_coords is None the dummy_grids won't be calculated.\n",
    "        If hashed is None the dummy_hashed won't be calculated.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid_coords : torch.Tensor or None, optional (default is None)\n",
    "            Grid coordinates.\n",
    "        hashed : torch.Tensor or None, optional (default is None)\n",
    "            Hashed grid coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[List[torch.Tensor] or None, List[torch.Tensor] or None, List[torch.Tensor] or None]\n",
    "            Dummy grids, original unique indices and dummy hashed .\n",
    "        \"\"\"\n",
    "\n",
    "        dummy_grids = None\n",
    "        og_indices = None\n",
    "        dummy_hashed = None\n",
    "\n",
    "        if grid_coords is not None:\n",
    "            dummy_grids: List[Tuple[torch.Tensor]] = [\n",
    "                # [\n",
    "                torch.unique(rearrange(grid_coords[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\"), dim=0, return_inverse=True) # first dimension is 0 because images should have all same size\n",
    "                for l in range(self._num_levels)\n",
    "                # ] for b in range(grid_coords.shape[0])\n",
    "            ]\n",
    "\n",
    "            dummy_grids, dummy_grids_inverse_indices = zip(*dummy_grids)\n",
    "            print2((\"dummy_grids:\", dummy_grids, [dummy_grids[l].shape for l in range(self._num_levels)]), (self._should_log > 1))\n",
    "            print2((\"dummy_grids grads:\", [(dummy_grids[l].requires_grad, ) for l in range(self._num_levels)]), (self._should_log > 3))\n",
    "\n",
    "            og_indices = [\n",
    "                torch.tensor([np.where(dummy_grids_inverse_indices[l].detach().cpu().numpy() == i)[0][0] for i in range(len(dummy_grids[l]))])\n",
    "                for l in range(self._num_levels)\n",
    "            ]\n",
    "            print2((\"og_indices:\", [(og_indices[l], og_indices[l].shape) for l in range(self._num_levels)]), (self._should_log > 1))\n",
    "            print2((\"og_indices grads:\", [(og_indices[l].requires_grad, ) for l in range(self._num_levels)]), (self._should_log > 1))\n",
    "\n",
    "        if hashed is not None:\n",
    "            dummy_hashed: List[torch.Tensor] = [\n",
    "                # [\n",
    "                torch.unique(rearrange(hashed[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\"), dim=0, return_inverse=False) # first dimension is 0 because images should have all same size\n",
    "                for l in range(self._num_levels)\n",
    "                # ] for b in range(hashed.shape[0])\n",
    "            ]\n",
    "            print2((\"dummy_hashed:\", dummy_hashed, [dummy_hashed[l].shape for l in range(self._num_levels)]), (self._should_log > 3))\n",
    "            print2((\"dummy_hashed grads:\", [(dummy_hashed[l].requires_grad, ) for l in range(self._num_levels)]), (self._should_log > 3))\n",
    "\n",
    "        return dummy_grids, og_indices, dummy_hashed \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calc_hash_collisions(\n",
    "        self, \n",
    "        dummy_grids: List[torch.Tensor] , \n",
    "        dummy_hashed: List[torch.Tensor] | None = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate hash collisions.\n",
    "        If dummy_hashed is None the min_possible_collision will be calculated while collisions won't.\n",
    "        If dummy_hashed is not None the min_possible_collision won't be calculated while collisions will.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dummy_grid_coords : List[torch.Tensor]\n",
    "            Grid coordinates.\n",
    "        dummy_hashed : List[torch.Tensor] or None, optional (default is None)\n",
    "            Hashed grid coordinates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor or None, torch.Tensor or None]\n",
    "            Minimum possible collisions and actual collisions at each level.\n",
    "        \"\"\"\n",
    "\n",
    "        min_possible_collisions = None\n",
    "        collisions = None\n",
    "\n",
    "        if dummy_hashed is None:\n",
    "            min_possible_collisions: torch.Tensor = torch.stack([\n",
    "                torch.tensor((dummy_grids[l].shape[0]) - self._hash_table_size)\n",
    "                for l in range(self._num_levels)\n",
    "            ])\n",
    "            min_possible_collisions[min_possible_collisions < 0] = 0\n",
    "            print2((\"min_possible_collisions:\", min_possible_collisions, min_possible_collisions.shape), (self._should_log > 2))\n",
    "            print2((\"min_possible_collisions grads:\", min_possible_collisions.requires_grad), (self._should_log > 2))\n",
    "        else:\n",
    "            collisions: torch.Tensor = torch.stack([\n",
    "                # [\n",
    "                torch.tensor(float(dummy_grids[l].shape[0] - torch.unique(dummy_hashed[l], dim=0).shape[0]))#, requires_grad=True)\n",
    "                for l in range(self._num_levels)\n",
    "                # ] \n",
    "                # for b in range(hashed.shape[0])\n",
    "            ])\n",
    "            print2((\"collisions:\", collisions, collisions.shape, collisions.dtype), (self._should_log > 2))\n",
    "            print2((\"collisions grads:\", collisions.requires_grad, ), (self._should_log > 2))\n",
    "\n",
    "        return min_possible_collisions, collisions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _calc_uniques(\n",
    "        self,\n",
    "        probs: torch.Tensor,\n",
    "        hashed: torch.Tensor,\n",
    "        sigmas: torch.Tensor,\n",
    "        og_indices: List[torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate uniques.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probs : torch.Tensor\n",
    "            Probabilities.\n",
    "        hashed : torch.Tensor\n",
    "            Hashed grid coordinates.\n",
    "        sigmas : torch.Tensor\n",
    "            Sigmas of hashed grid coordinates.\n",
    "        og_indices : List[torch.Tensor]\n",
    "            Original unique indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor]\n",
    "            Unique probabilities, unique hashed, and unique_sigmas.\n",
    "        \"\"\"\n",
    "\n",
    "        unique_probs: List[torch.Tensor] = [\n",
    "            # [\n",
    "            F.softmax(rearrange(probs[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\")[og_indices[l]], dim=0)\n",
    "            for l in range(self._num_levels)\n",
    "            # ]\n",
    "            # for b in range(probs.shape[0])\n",
    "        ]\n",
    "        print2((\"unique_probs:\", [(unique_probs[b][l], unique_probs[b][l].shape) for l in range(self._num_levels) for b in range(probs.shape[0])]), (self._should_log > 0))\n",
    "        print2((\"unique_probs:\", [(unique_probs[l], unique_probs[l].shape) for l in range(self._num_levels)]), (self._should_log > 0))\n",
    "\n",
    "        unique_hashed: List[torch.Tensor] = [\n",
    "            # [\n",
    "            rearrange(hashed[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\")[og_indices[l]]\n",
    "            for l in range(self._num_levels)\n",
    "            # ]\n",
    "            # for b in range(probs.shape[0])\n",
    "        ]\n",
    "        print2((\"unique_hashed:\", [(unique_hashed[b][l], unique_hashed[b][l].shape) for l in range(self._num_levels) for b in range(probs.shape[0])]), (self._should_log > 0))\n",
    "        print2((\"unique_hashed:\", [(unique_hashed[l], unique_hashed[l].shape) for l in range(self._num_levels)]), (self._should_log > 0))\n",
    "\n",
    "        unique_sigmas: List[torch.Tensor] = [\n",
    "            # [\n",
    "            rearrange(sigmas[0, :, l, :, :], \"pixels verts xyz -> (pixels verts) xyz\")[og_indices[l]]\n",
    "            for l in range(self._num_levels)\n",
    "            # ]\n",
    "            # for b in range(probs.shape[0])\n",
    "        ]\n",
    "        print2((\"unique_sigmas:\", [(unique_sigmas[b][l], unique_sigmas[b][l].shape) for l in range(self._num_levels) for b in range(probs.shape[0])]), (self._should_log > 0))\n",
    "        print2((\"unique_sigmas:\", [(unique_sigmas[l], unique_sigmas[l].shape) for l in range(self._num_levels)]), (self._should_log > 0))\n",
    "\n",
    "\n",
    "        return unique_probs, unique_hashed, unique_sigmas\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _hist_collisions(\n",
    "        self,\n",
    "        dummy_grids: List[torch.Tensor],\n",
    "        dummy_hashed: List[torch.Tensor],\n",
    "        min_possible_collisions: torch.Tensor,\n",
    "        should_show: bool = False\n",
    "    ) -> List[plt.Figure]:\n",
    "        \"\"\"\n",
    "        Show collisions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dummy_grids : List[torch.Tensor]\n",
    "            Grid coordinates.\n",
    "        dummy_hashed : List[torch.Tensor]\n",
    "            Hashed grid coordinates.\n",
    "        min_possible_collisions : torch.Tensor\n",
    "            Minimum possible collisions for each level.\n",
    "        should_show : bool, optional (default is False)\n",
    "            Whether to show figure.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[plt.Figure]\n",
    "            Histograms of collisions, one per level.\n",
    "        \"\"\"\n",
    "\n",
    "        figs=[]\n",
    "\n",
    "        # for l in range(self._num_levels):\n",
    "        for l, min_collisions in enumerate(min_possible_collisions):\n",
    "\n",
    "            if (min_collisions <= 0) and not self._should_fast_hash:\n",
    "                figs.append(None)\n",
    "                continue\n",
    "\n",
    "            indices = dummy_hashed[l].detach().cpu().numpy()\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(15, 5))\n",
    "            ax.hist(\n",
    "                indices,\n",
    "                bins=self._hash_table_size,\n",
    "                range=(0, self._hash_table_size),\n",
    "                edgecolor='grey', \n",
    "                linewidth=0.5\n",
    "            )\n",
    "\n",
    "            ax.set_xlim(-1, self._hash_table_size)\n",
    "            ax.xaxis.set_ticks(np.arange(0, self._hash_table_size, 10))\n",
    "\n",
    "            start, end = ax.get_ylim()\n",
    "            step = int(end * 0.1)\n",
    "            ax.yaxis.set_ticks(np.arange(0, end, step if step > 0 else 1))\n",
    "\n",
    "            plt.title(f\"Level {l} ({int(self._levels[0, 0, l, 0].item())})\")\n",
    "            plt.xlabel(\"Hashed indices\")\n",
    "            plt.ylabel(\"Counts\")\n",
    "\n",
    "            figs.append(fig)\n",
    "\n",
    "            if should_show:\n",
    "                plt.show()\n",
    "            \n",
    "            plt.close()\n",
    "\n",
    "        return figs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _fast_hash(self, grid: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Implements the hash function proposed by NVIDIA.\n",
    "        Args:\n",
    "            grid: A tensor of the shape (batch, pixels, levels, 2^input_dim, input_dim).\n",
    "               This tensor should contain the vertices of the hyper cube\n",
    "               for each level.\n",
    "        Returns:\n",
    "            A tensor of the shape (batch, pixels, levels, 2^input_dim, 1) containing the\n",
    "            indices into the hash table for all vertices.\n",
    "        '''\n",
    "        tmp = torch.zeros(\n",
    "            (grid.shape[0], grid.shape[1], self._num_levels, 2**self._input_dim),\n",
    "            dtype=torch.int64,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        for i in range(self._input_dim):\n",
    "            tmp = torch.bitwise_xor(\n",
    "                (grid[:, :, :, :, i].to(int) * self._prime_numbers[i]),\n",
    "                tmp\n",
    "            )\n",
    "\n",
    "        hash = torch.remainder(tmp, self._hash_table_size).unsqueeze(-1).to(float)\n",
    "        del tmp\n",
    "\n",
    "        return hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    x: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    l_collisions: float,\n",
    "    l_kl_loss: float,\n",
    "    l_sigma_loss: float,\n",
    "    l_zero_bins: float,\n",
    "    l_l2_reg: float = 0,\n",
    "    should_calc_hists: bool = False,\n",
    "    should_kl_hist: bool = False,\n",
    "    should_log: int = 0,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[plt.Figure], torch.Tensor, torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Train loop.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        Train input data.\n",
    "    model : nn.Module\n",
    "        Model to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer.\n",
    "    loss_fn : nn.Module\n",
    "        Loss function.\n",
    "    l_collisions : float\n",
    "        Collisions loss lambda.\n",
    "    l_kl_loss : float\n",
    "        KL loss lambda.\n",
    "    l_sigma_loss : float\n",
    "        Sigma loss lambda.\n",
    "    l_zero_bins : float\n",
    "        Zero bins loss lambda.\n",
    "    l_l2_reg : float, optional (default is 0)\n",
    "        L2 regularization lambda.\n",
    "    should_calc_hists : bool, optional (default is False)\n",
    "        Whether to calculate histograms or not.\n",
    "    should_kl_hist : bool, optional (default is False)\n",
    "        Whether to calculate KL with histograms or not.\n",
    "    should_log : int, optional (default is 0)\n",
    "        - 0: No logging.\n",
    "        - > 0: Log\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[plt.Figure], torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, float, float]\n",
    "        Output, unique probabilities, collisions, minimum possible collisions, histograms, collisions losses, KL losses, sigma loss, zero bins per level, l2_reg loss, loss.\n",
    "    \"\"\"\n",
    "    print2((\"Train loop\", ), should_log > 0, bcolors.WARNING)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out, unique_hashed, unique_probs, unique_sigmas, collisions, min_possible_collisions, hists = model(\n",
    "        x, \n",
    "        should_calc_hists=should_calc_hists \n",
    "    )\n",
    "\n",
    "    # print(\"out shape:\", out.shape)\n",
    "\n",
    "    # print(f\"Unique hashed: {[unique_hashed[l].shape for l in range(len(unique_hashed))]}\")\n",
    "\n",
    "    # print(f\"Softmax Unique hashed: {torch.softmax(unique_hashed[0].squeeze(-1), dim=-1), unique_hashed[0].shape}\")\n",
    "\n",
    "    # print(f\"Unique probs: {unique_probs[0], unique_probs[0].shape}\")\n",
    "\n",
    "    # print(\"out\", out)\n",
    "\n",
    "    collisions_losses, kl_div_losses, sigma_losses, zero_bins_per_level = loss_fn(\n",
    "        collisions, \n",
    "        min_possible_collisions, \n",
    "        unique_probs if not should_kl_hist else rearrange(out, \"batch pixels levels verts xyz -> batch levels pixels (verts xyz)\")[0], #unique_hashed,  \n",
    "        unique_sigmas\n",
    "    )\n",
    "\n",
    "    # L2 regularization\n",
    "    l2_reg_loss = torch.tensor(0.0)\n",
    "    for param in model.parameters():\n",
    "        l2_reg_loss += torch.norm(param)\n",
    "\n",
    "    collisions_loss = torch.sum(collisions_losses)\n",
    "    kl_loss = kl_div_losses.sum()\n",
    "    sigma_loss = sigma_losses.sum()\n",
    "    zero_bins_loss = zero_bins_per_level.sum()\n",
    "\n",
    "    loss = (\n",
    "        (l_collisions * collisions_loss) \n",
    "        + (l_kl_loss * kl_loss) \n",
    "        + (l_sigma_loss * sigma_loss) \n",
    "        + (l_zero_bins * zero_bins_loss)\n",
    "        + (l_l2_reg * l2_reg_loss)\n",
    "    )\n",
    "\n",
    "    print2((\"Loss grads:\", loss.requires_grad, ), should_log > 0, bcolors.HEADER)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return out, unique_probs, collisions, min_possible_collisions, hists, collisions_losses, kl_div_losses, sigma_losses, zero_bins_per_level, l2_reg_loss.item(), loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(\n",
    "    x: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    loss_fn: nn.Module,\n",
    "    l_collisions: float,\n",
    "    l_kl_loss: float,\n",
    "    l_sigma_loss: float,\n",
    "    l_zero_bins: float,\n",
    "    l_l2_reg: float = 0,\n",
    "    should_calc_hists: bool = False,\n",
    "    should_kl_hist: bool = False,\n",
    "    should_log: int = 0,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[plt.Figure], torch.Tensor, torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Test loop.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        Test input data.\n",
    "    model : nn.Module\n",
    "        Model to test.\n",
    "    loss_fn : nn.Module\n",
    "        Loss function.\n",
    "    l_collisions : float\n",
    "        Collisions loss lambda.\n",
    "    l_kl_loss : float\n",
    "        KL loss lambda.\n",
    "    l_sigma_loss : float\n",
    "        Sigma loss lambda.\n",
    "    l_zero_bins : float\n",
    "        Zero bins loss lambda.\n",
    "    l_l2_reg : float, optional (default is 0)\n",
    "        L2 regularization lambda.\n",
    "    should_calc_hists : bool, optional (default is False)\n",
    "        Whether to calculate histograms or not.\n",
    "    should_kl_hist : bool, optional (default is False)\n",
    "        Whether to calculate KL with histograms or not.\n",
    "    should_log : int, optional (default is 0)\n",
    "        - 0: No logging.\n",
    "        - > 0: Log\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[plt.Figure], torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, float, float]\n",
    "        Output, unique probabilities, collisions, minimum possible collisions, histograms, collisions losses, KL losses, sigma loss, zero bins per level, l2_reg loss, loss.\n",
    "    \"\"\"\n",
    "    print2((\"Test loop\", ), should_log > 0, bcolors.FAIL)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    out, unique_hashed, unique_probs, unique_sigmas, collisions, min_possible_collisions, hists = model(\n",
    "        x, \n",
    "        should_calc_hists=should_calc_hists \n",
    "    )\n",
    "\n",
    "    collisions_losses, kl_div_losses, sigma_losses, zero_bins_per_level = loss_fn(\n",
    "        collisions, \n",
    "        min_possible_collisions, \n",
    "        unique_probs if not should_kl_hist else rearrange(out, \"batch pixels levels verts xyz -> batch levels pixels (verts xyz)\")[0], #unique_hashed, \n",
    "        unique_sigmas\n",
    "    )\n",
    "\n",
    "    # L2 regularization\n",
    "    l2_reg_loss = torch.tensor(0.0)\n",
    "    for name, param in model.named_parameters():\n",
    "        if name != \"_prime_numbers\":\n",
    "            l2_reg_loss += torch.norm(param)\n",
    "\n",
    "    collisions_loss = torch.sum(collisions_losses)\n",
    "    kl_loss = kl_div_losses.sum()\n",
    "    sigma_loss = sigma_losses.sum()\n",
    "    zero_bins_loss = zero_bins_per_level.sum()\n",
    "\n",
    "    loss = (\n",
    "        (l_collisions * collisions_loss) \n",
    "        + (l_kl_loss * kl_loss) \n",
    "        + (l_sigma_loss * sigma_loss) \n",
    "        + (l_zero_bins * zero_bins_loss)\n",
    "        + (l_l2_reg * l2_reg_loss)\n",
    "    )\n",
    "\n",
    "    print2((\"Loss grads:\", loss.requires_grad, ), should_log > 0, bcolors.HEADER)\n",
    "\n",
    "    return out, unique_probs, collisions, min_possible_collisions, hists, collisions_losses, kl_div_losses, sigma_losses, zero_bins_per_level, l2_reg_loss.item(), loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics, Loss & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        delta: float = 1,\n",
    "        hash_table_size: int = 2**14,\n",
    "        should_kl_hist: bool = False,\n",
    "        should_diff_hist_optimized: bool = False,\n",
    "        should_use_all_levels: bool = False,\n",
    "        should_log: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Loss module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : float, optional (default is 1)\n",
    "            Delta parameter for collision loss.\n",
    "        hash_table_size : int, optional (default is 2**14)\n",
    "            Hash table size.\n",
    "        should_kl_hist : bool, optional (default is False)\n",
    "            Whether to calculate KL with histograms or not.\n",
    "        should_diff_hist_optimized: bool, optional (default is False)\n",
    "            Whether to use optimized differentiable histogram or not.\n",
    "        should_use_all_levels : bool, optional (default is False)\n",
    "            Whether to use all levels or only the ones with collisions.\n",
    "        should_log : int, optional (default is 0)\n",
    "            - 0: No logging.\n",
    "            - > 0: Log forward pass.\n",
    "            - > 1: Log helper functions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "        self._delta = delta\n",
    "        self._hash_table_size = hash_table_size\n",
    "        self._should_kl_hist = should_kl_hist\n",
    "        self._should_diff_hist_optimized = should_diff_hist_optimized\n",
    "        self._should_use_all_levels = should_use_all_levels\n",
    "        self._should_log = should_log\n",
    "\n",
    "        self.kl_div_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        collisions: List,\n",
    "        min_possible_collisions: List,\n",
    "        x: List, # levels pixels (verts xyz) \n",
    "        sigmas: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        delta = min_possible_collisions.clone()\n",
    "        delta[delta <= 0] = self._delta\n",
    "    \n",
    "        collisions_losses: torch.Tensor = (collisions - min_possible_collisions) / delta\n",
    "        print2((\"Collisions Losses:\", collisions_losses), self._should_log > 0)\n",
    "        print2((\"Collisions Losses grads:\", collisions_losses.requires_grad), self._should_log > 0)\n",
    "\n",
    "        zero_bins_per_level = torch.zeros(min_possible_collisions.shape)\n",
    "        kl_div_losses = torch.zeros(x.shape[0])\n",
    "\n",
    "        for l, level in enumerate(x):\n",
    "            if not self._should_use_all_levels and min_possible_collisions[l] <= 0:\n",
    "                continue\n",
    "            \n",
    "            p, zero_bins = self._calc_hist_pdf(level)\n",
    "            zero_bins_per_level[l] = zero_bins\n",
    "\n",
    "            kl_div_losses[l] = self._kl_div(l, p)\n",
    "\n",
    "        print2((\"KL Losses:\", kl_div_losses), self._should_log > 0, bcolors.FAIL)\n",
    "\n",
    "        sigma_losses = torch.stack([\n",
    "            self.mse_loss(sigma, torch.zeros_like(sigma))\n",
    "            for sigma in sigmas\n",
    "        ])\n",
    "\n",
    "        return collisions_losses, kl_div_losses, sigma_losses, zero_bins_per_level\n",
    "\n",
    "    def _calc_hist_pdf(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate histogram PDF.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Indices to which calculate histogram PDF.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "            Histogram PDF and count of zerobins.\n",
    "        \"\"\"\n",
    "        \n",
    "        print2((f\"x: {x}, shape: {x.shape}, requires_grad: {x.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "        hist_p_nondiff = torch.histc(x, bins=self._hash_table_size, min=0, max=self._hash_table_size) # ? maybe max=(self._hash_table_size - 1)\n",
    "        print2((f\"hist_p_nondiff: {hist_p_nondiff}, shape: {hist_p_nondiff.shape}, sum: {torch.sum(hist_p_nondiff)}, requires_grad: {hist_p_nondiff.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "        if self._should_diff_hist_optimized:\n",
    "            hist_p = torch.sum(histogram(x, bins=torch.torch.linspace(0, self._hash_table_size, self._hash_table_size), bandwidth=torch.tensor(0.5)), dim=0)\n",
    "        else:\n",
    "            hist_p = differentiable_histogram(x, bins=self._hash_table_size, min=0, max=self._hash_table_size).squeeze(0).squeeze(0)\n",
    "        print2((f\"hist_p_diff: {hist_p.long()}, shape: {hist_p.shape}, sum: {torch.sum(hist_p)}, requires_grad: {hist_p.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "        # assert torch.allclose(hist_p_nondiff, hist_p, atol=1), f\"hist_p_nondiff != hist_p: {(hist_p_nondiff - hist_p)}\"\n",
    "\n",
    "        p = hist_p / torch.prod(torch.tensor(x.shape))\n",
    "        print2((f\"p: {p}, shape: {p.shape}, requires_grad: {p.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "        zero_bins = torch.count_nonzero(p == 0)\n",
    "\n",
    "        p[p == 0] = 1e-10\n",
    "        print2((f\"after p: {p}, shape: {p.shape}, requires_grad: {p.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "        return p, zero_bins\n",
    "\n",
    "    def _kl_div(\n",
    "        self,\n",
    "        level: int,\n",
    "        p: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate KL divergence loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        level : int\n",
    "            Level.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            KL divergence loss.\n",
    "        \"\"\"\n",
    "\n",
    "        print2((f\"p: {p}, shape: {p.shape}, requires_grad: {p.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "        if self._should_kl_hist:\n",
    "            q = torch.ones(self._hash_table_size) / self._hash_table_size\n",
    "        else:\n",
    "            # q = torch.ones(level) / level\n",
    "            q = torch.arange(level, dtype=torch.float32)\n",
    "            print2((f\"before q: {q}, shape: {q.shape}, requires_grad: {q.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "            q = torch.softmax(q, dim=-1)\n",
    "\n",
    "        # print2((f\"after q: {q}, shape: {q.shape}, requires_grad: {q.requires_grad}\", ), self._should_log > 1)\n",
    "\n",
    "        return self.kl_div_loss(p.log(), q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(\n",
    "    net: torch.nn.Module,\n",
    "    # encoding_lr: float,\n",
    "    hash_lr: float,\n",
    "    # MLP_lr: float,\n",
    "    # encoding_weight_decay: float,\n",
    "    hash_weight_decay: float,\n",
    "    # MLP_weight_decay: float,\n",
    "    betas: tuple = (0.9, 0.99),\n",
    "    eps: float = 1e-15\n",
    "):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            # {\"params\": net.encoding.parameters(), \"lr\": encoding_lr, \"weight_decay\": encoding_weight_decay},\n",
    "            {\"params\": net.HashFunction.parameters(), \"lr\": hash_lr, \"weight_decay\": hash_weight_decay},\n",
    "            # {\"params\": net.mlp.parameters(), \"lr\": MLP_lr, \"weight_decay\": MLP_weight_decay}\n",
    "        ],\n",
    "        betas=betas,\n",
    "        eps=eps,\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, tolerance: int = 5, min_delta: int = 0, should_reset: bool = True):\n",
    "        self.tolerance: int = tolerance\n",
    "        self.min_delta: int = min_delta\n",
    "        self.best_loss: float = np.inf\n",
    "        self.counter: int = 0\n",
    "        self.early_stop: bool = False\n",
    "        self._should_reset: bool = should_reset\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        # print(f\"best_loss: {self.best_loss}, loss: {loss}, counter: {self.counter}\")\n",
    "\n",
    "        if abs(self.best_loss - loss) < self.min_delta and (loss < self.best_loss):\n",
    "            # print(\"Stall\")\n",
    "            self.counter += 1\n",
    "        elif abs(self.best_loss - loss) > self.min_delta and (loss > self.best_loss):\n",
    "            # print(\"Growing\")\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            if not self._should_reset:\n",
    "                if self.counter <= 0:\n",
    "                    self.counter = 0\n",
    "                else:\n",
    "                    self.counter -= 1\n",
    "            else:\n",
    "                self.counter = 0\n",
    "                self.best_loss = loss\n",
    "\n",
    "        if self.counter >= self.tolerance:\n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "root_dir = \"./images\"\n",
    "test_size = 0.2\n",
    "\n",
    "train_images, test_images = train_test_split(\n",
    "    [\n",
    "        file for file in os.listdir(root_dir) if (\"silhouette\" in file) and (file.endswith(\".jpg\") or file.endswith(\".png\") or file.endswith(\".jpeg\"))\n",
    "    ], \n",
    "    test_size=test_size, \n",
    "    random_state=65535#random_seed\n",
    ")\n",
    "\n",
    "wandb_entity = \"fedemonti00\"\n",
    "wandb_project = \"project_course\"\n",
    "# wandb_name = \"hash_function_training\"\n",
    "\n",
    "try:\n",
    "    time = wandb_name\n",
    "except NameError:\n",
    "    time = (datetime.now(ZoneInfo(\"Europe/Rome\"))).strftime(\"%Y%m%d%H%M%S\")\n",
    "print(\"RUN:\", time)\n",
    "\n",
    "histogram_rate = 10\n",
    "\n",
    "scheduler = None # None, StepLR or CosineAnnealingLR or CosineAnnealingWarmRestarts\n",
    "\n",
    "hyperparameters = {\n",
    "    \"hash_table_size\": 2**8,\n",
    "    \"n_min\": 8,\n",
    "    \"n_max\": 32,\n",
    "    \"num_levels\": 4,\n",
    "    \"output_dim\": 2,\n",
    "    \"hash_function_hidden_layers_widths\": [8, 32, 8],\n",
    "    \"hash_lr\": 1e-3,\n",
    "    \"hash_weight_decay\": 1e-6,\n",
    "    \"sigma_scale\": 1,\n",
    "    \"kl_hist_loss\": True,\n",
    "    \"differentiable_histogram_optimized\": False,\n",
    "    \"should_use_all_levels\": False, # !! should be true if NOT using kl_hist_loss !!\n",
    "    \"should_random_permute_input\": False,\n",
    "    \"should_normalize_levels\": False,\n",
    "    \"lambdas_decay\": -1, # -1 to disable\n",
    "    \"scheduler\": scheduler,\n",
    "    \"scheduler_gamma\": None if not scheduler else (0.9 if scheduler == \"StepLR\" else 1e-4),\n",
    "    \"l_collisions\": 0,\n",
    "    \"l_kl_loss\": 1e2,\n",
    "    \"l_sigma_loss\": 0,\n",
    "    \"l_zero_bins\": 0, # 1e-1,\n",
    "    \"l_l2_reg\": 0,\n",
    "    # \"epochs\": 2000,\n",
    "    # \"epochs\": 999,\n",
    "    \"epochs\": 1,\n",
    "    \"random_seed\": random_seed,\n",
    "}\n",
    "\n",
    "early_stopper_tolerance = hyperparameters[\"epochs\"] // 4\n",
    "early_stopper_min_delta = 1e-6 \n",
    "\n",
    "# should_log = False\n",
    "should_log = True\n",
    "should_wandb = True if hyperparameters[\"epochs\"] > 999 else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_wandb:\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        entity = wandb_entity,\n",
    "        # set the wandb project where this run will be logged\n",
    "        project = wandb_project,\n",
    "\n",
    "        name = time,\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config = hyperparameters,\n",
    "        # config = {\n",
    "        #     \"id_grid_search_params\":        id_param,\n",
    "        #     \"grid_search_params\":           params,\n",
    "        #     \"random_seed\":                  random_seed,\n",
    "        #     \"HPD_learning_rate\":            HPD_lr,\n",
    "        #     \"encoding_learning_rate\":       encoding_lr,\n",
    "        #     \"MLP_learning_rate\":            MLP_lr,\n",
    "        #     \"encoding_weight_decay\":        encoding_weight_decay,\n",
    "        #     \"HPD_weight_decay\":             HPD_weight_decay,\n",
    "        #     \"MLP_weight_decay\":             MLP_weight_decay,\n",
    "        #     \"batch_size%\":                  batch_size,\n",
    "        #     \"shuffled_pixels\":              should_shuffle_pixels,\n",
    "        #     \"normalized_data\":              True if not should_batchnorm_data else \"BatchNorm1d\",\n",
    "        #     \"architecture\":                 \"GeneralNeuralGaugeFields\",\n",
    "        #     \"dataset\":                      image_name,\n",
    "        #     \"epochs\":                       epochs,\n",
    "        #     \"color\":                        'RGB' if not should_bw else 'BW',\n",
    "        #     \"hash_table_size\":              hash_table_size,\n",
    "        #     \"num_levels\":                   num_levels,\n",
    "        #     \"n_min\":                        n_min,\n",
    "        #     \"n_max\":                        n_max,\n",
    "        #     \"MLP_hidden_layers_widths\":     str(MLP_hidden_layers_widths),\n",
    "        #     \"HPD_hidden_layers_widths\":     str(HPD_hidden_layers_widths),\n",
    "        #     \"HPD_out_features\":             HPD_out_features,\n",
    "        #     \"feature_dim\":                  feature_dim,\n",
    "        #     \"topk_k\":                       topk_k,\n",
    "        #     \"loss_type\":                    \"JS+KLDiv\" if should_sum_js_kl_div else (\"KLDiv\" if not should_js_div else \"JSDiv\"),\n",
    "        #     \"loss_lambda_MSE\":              l_mse,\n",
    "        #     \"loss_lambda_JS_KL\":            l_js_kl,\n",
    "        #     \"loss_lambda_collisions\":       l_collisions,\n",
    "        #     \"loss_gamma\":                   loss_gamma,\n",
    "        #     \"loss_epsilon\":                 loss_epsilon,\n",
    "        #     \"inplace_scatter\":              should_inplace_scatter,\n",
    "        #     \"MLP_activations\":              \"LeakyReLU\" if should_leaky_relu else \"ReLU\",\n",
    "        #     \"collisions_loss_probs\":        \"topk_only\" if should_keep_topk_only else \"hash_table_size\",\n",
    "        #     \"avg_topk_features\":            \"softmax_avg\" if should_softmax_topk_features else (\"weighted_avg\" if should_softmax_topk_features != None else None),\n",
    "        #     \"hash_type\":                    \"HPD\" if not should_use_hash_function else \"hash_function\"\n",
    "        # }\n",
    "\n",
    "        save_code = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_log(\n",
    "    e: int,\n",
    "    lr: float,\n",
    "    l_zero_bins: float,\n",
    "    l_kl_loss: float,\n",
    "\n",
    "    train: Tuple = None,\n",
    "    test: Tuple = None,\n",
    "    fast_hash: Tuple = None,\n",
    "\n",
    "    # train_loss: float,\n",
    "    # train_collisions: torch.Tensor,\n",
    "    # train_min_possible_collisions: torch.Tensor,\n",
    "    # train_collisions_losses: torch.Tensor,\n",
    "    # train_kl_losses: torch.Tensor,\n",
    "    # train_sigma_losses: torch.Tensor,\n",
    "    # train_l2_reg_loss: float,\n",
    "    # train_hists: List[plt.Figure],\n",
    "    # train_zero_bins:torch.Tensor,\n",
    "    # test_loss: float,\n",
    "    # test_collisions: torch.Tensor,\n",
    "    # test_min_possible_collisions: torch.Tensor,\n",
    "    # test_collisions_losses: torch.Tensor,\n",
    "    # test_kl_losses: torch.Tensor,\n",
    "    # test_sigma_losses: torch.Tensor,\n",
    "    # test_l2_reg_loss: float,\n",
    "    # test_zero_bins:torch.Tensor,\n",
    "    # test_hists: List[plt.Figure],\n",
    "    should_log_hists: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Log to wandb.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    e : int\n",
    "        Epoch.\n",
    "    lr : float\n",
    "        Learning rate modified by the scheduler.\n",
    "    l_zero_bins : float\n",
    "        Zero bins loss lambda.\n",
    "    l_kl_loss : float\n",
    "        KL loss lambda.\n",
    "    train_loss : float\n",
    "        Train loss.\n",
    "    train_collisions : torch.Tensor\n",
    "        Train collisions.\n",
    "    train_min_possible_collisions : torch.Tensor\n",
    "        Train minimum possible collisions.\n",
    "    train_collisions_losses : torch.Tensor\n",
    "        Train collisions losses.\n",
    "    train_kl_losses : torch.Tensor\n",
    "        Train KL losses.\n",
    "    train_sigma_losses : torch.Tensor\n",
    "        Train sigma losses.\n",
    "    train_l2_reg_loss : float\n",
    "        Train L2 regularization loss.\n",
    "    train_zero_bins : torch.Tensor\n",
    "        Train zero bins.\n",
    "    train_hists : List[plt.Figure]\n",
    "        Train histograms.\n",
    "    test_loss : float\n",
    "        Test loss.\n",
    "    test_collisions : torch.Tensor\n",
    "        Test collisions.\n",
    "    test_min_possible_collisions : torch.Tensor\n",
    "        Test minimum possible collisions.\n",
    "    test_collisions_losses : torch.Tensor\n",
    "        Test collisions losses.\n",
    "    test_kl_losses : torch.Tensor\n",
    "        Test KL losses.\n",
    "    test_sigma_losses : torch.Tensor\n",
    "        Test sigma losses.\n",
    "    test_l2_reg_loss : float\n",
    "        Test L2 regularization loss.\n",
    "    test_zero_bins : torch.Tensor\n",
    "        Test zero bins.\n",
    "    test_hists : List[plt.Figure]\n",
    "        Test histograms.\n",
    "    should_log_hists : bool, optional (default is False)\n",
    "        Whether to log histograms or not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    if fast_hash is not None:\n",
    "        _, _, fast_hash_collisions, fast_hash_min_possible_collisions, fast_hash_hists, fast_hash_collisions_losses, fast_hash_kl_losses, fast_hash_sigma_losses, fast_hash_zero_bins, fast_hash_l2_reg_loss, fast_hash_loss = fast_hash\n",
    "        log = {}\n",
    "        log[\"fast_hash/loss\"] = fast_hash_loss\n",
    "        log[\"fast_hash/l2_reg_loss\"] = fast_hash_l2_reg_loss\n",
    "        \n",
    "        for l in range(hyperparameters[\"num_levels\"]):\n",
    "            log[f\"fast_hash/collisions_level_{l}\"] = fast_hash_collisions[l].item()\n",
    "            log[f\"fast_hash/min_possible_collisions_level_{l}\"] = fast_hash_min_possible_collisions[l].item()\n",
    "            log[f\"fast_hash/collisions_loss_level_{l}\"] = fast_hash_collisions_losses[l].item()\n",
    "            log[f\"fast_hash/kl_loss_level_{l}\"] = fast_hash_kl_losses[l].item()\n",
    "            log[f\"fast_hash/sigma_loss_level_{l}\"] = fast_hash_sigma_losses[l].item()\n",
    "            log[f\"fast_hash/zero_bins_level_{l}\"] = fast_hash_zero_bins[l].item()\n",
    "        \n",
    "            if should_log_hists and fast_hash_hists[l] is not None:\n",
    "                log[f\"fast_hash/hist_counts_level_{l}\"] = wandb.Image(\n",
    "                    fast_hash_hists[l],\n",
    "                    caption=f\"Hashed indices counts at level {l} at epoch {e}\"\n",
    "                )\n",
    "    else:\n",
    "        _, _, train_collisions, train_min_possible_collisions, train_hists, train_collisions_losses, train_kl_losses, train_sigma_losses, train_zero_bins, train_l2_reg_loss, train_loss = train\n",
    "        _, _, test_collisions, test_min_possible_collisions, test_hists, test_collisions_losses, test_kl_losses, test_sigma_losses, test_zero_bins, test_l2_reg_loss, test_loss = test\n",
    "\n",
    "        log = {\n",
    "            \"train/loss\": train_loss,\n",
    "            \"test/loss\": test_loss,\n",
    "            \"train/l2_reg_loss\": train_l2_reg_loss,\n",
    "            \"test/l2_reg_loss\": test_l2_reg_loss,\n",
    "        }\n",
    "\n",
    "        if hyperparameters[\"lambdas_decay\"] > -1:\n",
    "            log[\"l_zero_bins\"] = l_zero_bins\n",
    "            log[\"l_kl_loss\"] = l_kl_loss\n",
    "\n",
    "        if hyperparameters[\"scheduler\"] is not None:\n",
    "            log[\"lr\"] = lr\n",
    "\n",
    "        for l in range(hyperparameters[\"num_levels\"]):\n",
    "            log[f\"train/collisions_level_{l}\"] = train_collisions[l].item()\n",
    "            log[f\"train/min_possible_collisions_level_{l}\"] = train_min_possible_collisions[l].item()\n",
    "            log[f\"train/collisions_loss_level_{l}\"] = train_collisions_losses[l].item()\n",
    "            log[f\"train/kl_loss_level_{l}\"] = train_kl_losses[l].item()\n",
    "            log[f\"train/sigma_loss_level_{l}\"] = train_sigma_losses[l].item()\n",
    "            log[f\"train/zero_bins_level_{l}\"] = train_zero_bins[l].item()\n",
    "\n",
    "            log[f\"test/collisions_level_{l}\"] = test_collisions[l].item()\n",
    "            log[f\"test/min_possible_collisions_level_{l}\"] = test_min_possible_collisions[l].item()\n",
    "            log[f\"test/collisions_loss_level_{l}\"] = test_collisions_losses[l].item()\n",
    "            log[f\"test/kl_loss_level_{l}\"] = test_kl_losses[l].item()\n",
    "            log[f\"test/sigma_loss_level_{l}\"] = test_sigma_losses[l].item()\n",
    "            log[f\"test/zero_bins_level_{l}\"] = test_zero_bins[l].item()\n",
    "\n",
    "            \n",
    "            if should_log_hists and train_hists[l] is not None:\n",
    "                log[f\"train/hist_counts_level_{l}\"] = wandb.Image(\n",
    "                    train_hists[l],\n",
    "                    caption=f\"Hashed indices counts at level {l} at epoch {e}\"\n",
    "                )\n",
    "            \n",
    "            if should_log_hists and test_hists[l] is not None:\n",
    "                log[f\"test/hist_counts_level_{l}\"] = wandb.Image(\n",
    "                    test_hists[l],\n",
    "                    caption=f\"Hashed indices counts at level {l} at epoch {e}\"\n",
    "                )\n",
    "    \n",
    "    wandb.log(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImagesDataset(\n",
    "    root=root_dir.split(\"/\")[0],\n",
    "    dir_name=root_dir.split(\"/\")[1],\n",
    "    images_names=train_images,\n",
    "    should_random_permute_input=hyperparameters[\"should_random_permute_input\"]\n",
    ")\n",
    "x, y, h, w, reordered_indices, names = train_dataset[-1]\n",
    "\n",
    "test_dataset = ImagesDataset(\n",
    "    root=root_dir.split(\"/\")[0],\n",
    "    dir_name=root_dir.split(\"/\")[1],\n",
    "    images_names=[\"strawberry_small.jpg\"],#test_images\n",
    "    # should_random_permute_input=hyperparameters[\"should_random_permute_input\"]\n",
    ")\n",
    "eval_x, eval_y, eval_h, eval_w, _, eval_names = test_dataset[-1]\n",
    "\n",
    "input_dim = x.shape[-3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashFunction = HashFunction(\n",
    "    hidden_layers_widths=hyperparameters[\"hash_function_hidden_layers_widths\"],\n",
    "    input_dim=input_dim,\n",
    "    output_dim=hyperparameters[\"output_dim\"],\n",
    "    hash_table_size=hyperparameters[\"hash_table_size\"],\n",
    "    sigma_scale=hyperparameters[\"sigma_scale\"],\n",
    "    should_log=0 if should_log else 0\n",
    ")\n",
    "\n",
    "multires = Multiresolution(\n",
    "    n_min=hyperparameters[\"n_min\"],\n",
    "    n_max=hyperparameters[\"n_max\"],\n",
    "    num_levels=hyperparameters[\"num_levels\"],\n",
    "    HashFunction=hashFunction,\n",
    "    hash_table_size=hyperparameters[\"hash_table_size\"],\n",
    "    input_dim=input_dim,\n",
    "    should_use_all_levels=hyperparameters[\"should_use_all_levels\"],\n",
    "    should_normalize_levels=hyperparameters[\"should_normalize_levels\"],\n",
    "    should_log=0 if should_log else 0\n",
    ")\n",
    "\n",
    "multires_fast_hash = Multiresolution(\n",
    "    n_min=hyperparameters[\"n_min\"],\n",
    "    n_max=hyperparameters[\"n_max\"],\n",
    "    num_levels=hyperparameters[\"num_levels\"],\n",
    "    HashFunction=None,\n",
    "    hash_table_size=hyperparameters[\"hash_table_size\"],\n",
    "    input_dim=input_dim,\n",
    "    should_use_all_levels=True,\n",
    "    should_normalize_levels=hyperparameters[\"should_normalize_levels\"],\n",
    "    should_fast_hash=True,\n",
    "    should_log=0 if should_log else 0\n",
    ")\n",
    "\n",
    "loss_fn = Loss(\n",
    "    hash_table_size=hyperparameters[\"hash_table_size\"],\n",
    "    should_kl_hist=hyperparameters[\"kl_hist_loss\"],\n",
    "    should_diff_hist_optimized=hyperparameters[\"differentiable_histogram_optimized\"],\n",
    "    should_use_all_levels=hyperparameters[\"should_use_all_levels\"],\n",
    "    should_log=0 if should_log else 0\n",
    ")\n",
    "\n",
    "optimizer = get_optimizer(\n",
    "    net=multires,\n",
    "    hash_lr=hyperparameters[\"hash_lr\"],\n",
    "    hash_weight_decay=hyperparameters[\"hash_weight_decay\"],\n",
    ")\n",
    "\n",
    "if hyperparameters[\"epochs\"] > 1:\n",
    "    if hyperparameters[\"scheduler\"] == \"CosineAnnealingLR\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=hyperparameters[\"epochs\"]//4, eta_min=hyperparameters[\"scheduler_gamma\"])\n",
    "    elif hyperparameters[\"scheduler\"] == \"CosineAnnealingWarmRestarts\":\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=hyperparameters[\"epochs\"]//4, T_mult=1, eta_min=hyperparameters[\"scheduler_gamma\"])\n",
    "    elif hyperparameters[\"scheduler\"] == \"StepLR\":\n",
    "        scheduler = StepLR(optimizer, step_size=int(0.05 * hyperparameters[\"epochs\"]), gamma=hyperparameters[\"scheduler_gamma\"])\n",
    "\n",
    "early_stopper = EarlyStopper(\n",
    "    tolerance=early_stopper_tolerance,\n",
    "    min_delta=early_stopper_min_delta\n",
    ")\n",
    "\n",
    "print(multires)\n",
    "print(multires_fast_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "should_calc_hists = False\n",
    "\n",
    "pbar = tqdm(range(0, hyperparameters[\"epochs\"]))\n",
    "\n",
    "# _, _, fast_hash_collisions, fast_hash_min_possible_collisions, fast_hash_hists, fast_hash_collisions_losses, fast_hash_kl_losses, fast_hash_sigma_losses, fast_hash_zero_bins, fast_hash_l2_reg_loss, fast_hash_loss = test_loop(\n",
    "fast_hash = test_loop(\n",
    "    x=eval_x,\n",
    "    model=multires_fast_hash,\n",
    "    loss_fn=loss_fn,\n",
    "    l_l2_reg=hyperparameters[\"l_l2_reg\"],\n",
    "    l_collisions=hyperparameters[\"l_collisions\"],\n",
    "    l_kl_loss=hyperparameters[\"l_kl_loss\"],\n",
    "    l_sigma_loss=hyperparameters[\"l_sigma_loss\"],\n",
    "    l_zero_bins=hyperparameters[\"l_zero_bins\"],\n",
    "    # Calc histograns at: first epoch, last epoch, every histogram_rate epochs, every time early stopper stops\n",
    "    should_calc_hists=True,\n",
    "    should_kl_hist=hyperparameters[\"kl_hist_loss\"],\n",
    "    should_log=1 if should_log else 0\n",
    ")\n",
    "\n",
    "if should_wandb:\n",
    "    wandb_log(\n",
    "        e=-1,\n",
    "        lr=None,\n",
    "        l_zero_bins=hyperparameters[\"l_zero_bins\"],\n",
    "        l_kl_loss=hyperparameters[\"l_kl_loss\"],\n",
    "        fast_hash=fast_hash,\n",
    "        should_log_hists=True\n",
    "    )\n",
    "\n",
    "for e in pbar:\n",
    "    should_calc_hists = ((e == hyperparameters[\"epochs\"] - 1) or (e % histogram_rate == 0) or early_stopper.early_stop)\n",
    "    \n",
    "    # _, _, train_collisions, train_min_possible_collisions, train_hists, train_collisions_losses, train_kl_losses, train_sigma_losses, train_zero_bins, train_l2_reg_loss, train_loss = train_loop(\n",
    "    train = train_loop(\n",
    "        x=x,\n",
    "        model=multires,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        l_collisions=hyperparameters[\"l_collisions\"],\n",
    "        l_kl_loss=hyperparameters[\"l_kl_loss\"],\n",
    "        l_sigma_loss=hyperparameters[\"l_sigma_loss\"],\n",
    "        l_zero_bins=hyperparameters[\"l_zero_bins\"],\n",
    "        l_l2_reg=hyperparameters[\"l_l2_reg\"],\n",
    "        # Calc histograns at: first epoch, last epoch, every histogram_rate epochs, every time early stopper stops\n",
    "        should_calc_hists=should_calc_hists,\n",
    "        should_kl_hist=hyperparameters[\"kl_hist_loss\"],\n",
    "        should_log=1 if should_log else 0\n",
    "    )\n",
    "\n",
    "    # _, _, test_collisions, test_min_possible_collisions, test_hists, test_collisions_losses, test_kl_losses, test_sigma_losses, test_zero_bins, test_l2_reg_loss, test_loss = test_loop(\n",
    "    test = test_loop(\n",
    "        x=eval_x,\n",
    "        model=multires,\n",
    "        loss_fn=loss_fn,\n",
    "        l_l2_reg=hyperparameters[\"l_l2_reg\"],\n",
    "        l_collisions=hyperparameters[\"l_collisions\"],\n",
    "        l_kl_loss=hyperparameters[\"l_kl_loss\"],\n",
    "        l_sigma_loss=hyperparameters[\"l_sigma_loss\"],\n",
    "        l_zero_bins=hyperparameters[\"l_zero_bins\"],\n",
    "        # Calc histograns at: first epoch, last epoch, every histogram_rate epochs, every time early stopper stops\n",
    "        should_calc_hists=should_calc_hists,\n",
    "        should_kl_hist=hyperparameters[\"kl_hist_loss\"],\n",
    "        should_log=1 if should_log else 0\n",
    "    )\n",
    "\n",
    "    if should_wandb:\n",
    "        wandb_log(\n",
    "            e=e,\n",
    "            lr=scheduler.get_last_lr()[0] if hyperparameters[\"scheduler\"] is not None else None, \n",
    "            l_zero_bins=hyperparameters[\"l_zero_bins\"],\n",
    "            l_kl_loss=hyperparameters[\"l_kl_loss\"],\n",
    "            train=train,\n",
    "            test=test,\n",
    "            # train_loss=train_loss,\n",
    "            # train_collisions=train_collisions,\n",
    "            # train_min_possible_collisions=train_min_possible_collisions,\n",
    "            # train_collisions_losses=train_collisions_losses,\n",
    "            # train_kl_losses=train_kl_losses,\n",
    "            # train_sigma_losses=train_sigma_losses,\n",
    "            # train_l2_reg_loss=train_l2_reg_loss,\n",
    "            # train_zero_bins=train_zero_bins,\n",
    "            # train_hists=train_hists,\n",
    "            # test_loss=test_loss,\n",
    "            # test_collisions=test_collisions,\n",
    "            # test_min_possible_collisions=test_min_possible_collisions,\n",
    "            # test_collisions_losses=test_collisions_losses,\n",
    "            # test_kl_losses=test_kl_losses,\n",
    "            # test_sigma_losses=test_sigma_losses,\n",
    "            # test_l2_reg_loss=test_l2_reg_loss,\n",
    "            # test_zero_bins=test_zero_bins,\n",
    "            # test_hists=test_hists,\n",
    "            should_log_hists=should_calc_hists\n",
    "        )\n",
    "\n",
    "    if (hyperparameters[\"lambdas_decay\"] > -1) and e != 0 and e % hyperparameters[\"lambdas_decay\"] == 0:\n",
    "        hyperparameters[\"l_zero_bins\"] = hyperparameters[\"l_zero_bins\"] * 0.9\n",
    "        hyperparameters[\"l_kl_loss\"] = hyperparameters[\"l_kl_loss\"] * 1.001\n",
    "    \n",
    "    if hyperparameters[\"scheduler\"] is not None and hyperparameters[\"epochs\"] > 1:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # if np.isnan(train_loss):\n",
    "    if np.isnan(train[-1]):\n",
    "        break \n",
    "\n",
    "    # pbar.set_description(f\"Epoch {e}, Loss: {test_loss}, Collisions: {test_collisions}\")\n",
    "    pbar.set_description(f\"Epoch {e}, Loss: {test[-1]}, Collisions: {test[2]}\")\n",
    "\n",
    "    # for name, param in multires.named_parameters():\n",
    "    #     print(f'Parameter: {name}, Gradient: {param.grad}')\n",
    "\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"!!! Stopping at epoch:\", e, \"!!!\")\n",
    "        break\n",
    "\n",
    "    # early_stopper(test_loss)\n",
    "    early_stopper(test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_wandb:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_course",
   "language": "python",
   "name": "project_course"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
